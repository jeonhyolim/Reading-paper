{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSfah6SLfhVG"
      },
      "source": [
        "# 텍스트 생성 방법 : Transformers를 이용한 언어생성에 서로 다른 디코딩 방법 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzZ_IVTtoQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aabb23f-5b1a-44a1-e526-19249f28e3e9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l31dGxwgTMZ"
      },
      "source": [
        "# 개요\n",
        "가장 두드러진 decoding 방법으로 Greedy search, Beam search, Top-K sampling, Top-p sampling 살펴보기\n",
        "\n",
        "https://colab.research.google.com/drive/1yUGVmQ0nj8Hd3h0YV6PemQx0FtzpefGB?usp=sharing를 일부 수정하였습니다\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz0J-hPCoZEl"
      },
      "source": [
        "Transformers를 설치하고 Model을 load하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4jS5zNKknHT"
      },
      "source": [
        "이번 실습을 위해 SKT에서 공개한 KoGPT-2 모델을 사용해보도록 하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1eVsFQQgdBk",
        "outputId": "e390f25a-ae83-4b33-9187-5ecb08ab9b8a"
      },
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 1.6.14\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.2.0).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "Git LFS initialized.\n",
            "fatal: destination path 'kogpt2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer # 라이브러리에서 사용할 tokenizer를 import\n",
        "from transformers import GPT2Config, GPT2LMHeadModel # transformers 라이브러리에서 사용할 언어 모델을 import\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")  # 입력 문장을 토큰 단위로 쪼개는 역할\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xnLnlm0DBO4",
        "outputId": "1a35a4e3-f722-42ce-8706-ccf424af4402"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=50000, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckGerDmxgjOb",
        "outputId": "892ebdfa-7452-40c1-aa07-2e03df77d588"
      },
      "source": [
        "config = GPT2Config(vocab_size=50000)\n",
        "config.pad_token_id = tokenizer.token_to_id('<pad>')\n",
        "model = GPT2LMHeadModel(config) # 쪼개진 입력 문장을 가지고 실제 문장생성을 진행\n",
        "model\n",
        "\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50000, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYdGXD4opfuk"
      },
      "source": [
        "### **Greedy Search**\n",
        "\n",
        "Greedy search는 단순히 가장 높은 확률을 가진 단어를 다음 단어로 선택합니다.   \n",
        "$w_t = argmax_{w}P(w | w_{1:t-1})$ 는 각각의 timestep $t$ 입니다. 아래 그림은 greedy search을 보여줍니다.   \n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
        "\n",
        "알고리즘은 단어 \"The\"에서 시작하여 다음 단어로 가장 높은 확률의 단어인 \"nice\" 등을 선택하는 탐욕법입니다. 그러므로 최종적으로 생성된 Word sequence는 \"The\", \"nice\", \"woman\"이며 전반적인 확률은 0.5x0.4 = 0.2로 계산됩니다.\n",
        "\n",
        "다음 문맥 (\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\")에서 GPT2를 사용하여 Word sequence를 생성할 수 있습니다.\n",
        "\n",
        "##Transformers에서 greedy search를 사용하는 방법\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLd_J6lXz_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d565a92-a7df-4e50-ab19-00ad89f64623"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "def tokenizing(text):\n",
        "    return torch.tensor(tokenizer.encode(text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "input_ids = tokenizing(\"이순신은 조선 중기의 무신이다.\")\n",
        "\n",
        "# 생성 모델은 generate 함수를 통해 다음 token을 생성 가능.\n",
        "greedy_output = model.generate(   # 이 안에 들어가는 값들이 decoding 옵션들\n",
        "        nput_ids, max_length=50) # generate text 는 context length를 포함해서 max_length에 도달하기 전까지 문장을 생성해낸다\n",
        "\n",
        "print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 작곡가 겸 가수인 윤도현의 곡들을 모은 것이다.</s><s> 이 목록은 대한민국의 작곡가 겸 가수인 윤도현의 곡들을 모은 것이다.</s><s> 이 목록은 대한민국의 작곡가 겸 가수인\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNIDMgkds-VO"
      },
      "source": [
        "GPT2로 짧은 텍스트를 생성.   \n",
        "생성된 단어 문맥은 합리적이지만 모델은 비슷한 단어를 반복하는 수준으로, 이러한 현상은 일반적인 언어생성 모델에서 나타나는 공통된 문제로 특히 Greedy search와 Beam search에서 훨씬 자주 나타난다\n",
        "\n",
        "Greedy search의 주요 단점은 (그림에서 볼수 있듯이) 낮은 확률 단어 이후에 나올수 있는 더 높은 확률의 단어를 놓친다는 것이다.\n",
        "\n",
        "예를 들면 단어 \"has\"는 0.9의 높은 조건부 확률을 가지고 있지만, 첫 검색단어중 두번째로 높은 조건부 확률 단어인 \"dog\" 이후에  숨어있는 형태이어서 Greedy search는 \"The\",\"dog\",\"has\"라는 Word sequence를 놓치게 된다.\n",
        "\n",
        "이러한 문제는 Beam search에서 완화 가능하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y73RcrRPn-Bn"
      },
      "source": [
        "### **Beam search**\n",
        "\n",
        "Beam search는 각 Time step에서 가장 확률이 높은 Hypotheses의 num_beams를 유지하고 결국 전체 확률이 가장 높은 hypothesis를 선택하는 것으로 숨겨진 높은 확률 Word sequence를 놓칠 위험을 줄입니다.\n",
        "\n",
        "`num_beams =2`라고 가정하고 Toy example을 보면\n",
        "\n",
        "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
        "\n",
        "Time step=1일때, Beam search는 가장 가능성 높은 Hypothesis \"The\",\"woman\"외에도 두번째로 가능성 높은 Hypothesis인 \"The\",\"dog\"를 추적. \n",
        "\n",
        "Time step=2일때, Beam search는 Word sequence 확률 0.2를 가진 (\"The\",\"nice\",\"woman\") 보다 확률 0.36을 가진 (\"The\", \"dog\", \"has\")가 높다는 것을 찾습니다. 이것으로 Toy example에서 가장 가능성 높은 Word sequence를 발견 할 수 있다는 것을 보임.\n",
        "\n",
        "Beam search는 항상 Greedy search보다 높은 확률의 결과 Sequence를 찾는 것이 가능. 그러나 이것이 가장 가능성 높은 결과를 찾은 것이라고는 보장할 수 없음. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1R5kx30Ynej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33631a22-6d6d-469a-b6d1-1e873f216271"
      },
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    input_ids,  \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신은 조선 중기의 무신이다.</s><s> 그 후, 조선이 멸망한 후, 조선을 건국하였다.</s><s> 그 후, 조선 건국 후, 조선을 건국하여 건국하였다.</s><s> 조선 건국 후, 조선을 건국하여 건국\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCeY_atbBMr7"
      },
      "source": [
        "동일한 Word sequence를 반복하는 문제.\n",
        "\n",
        " n-grams 패널티를 도입해, 이미 나타난 n-gram에 대해 다음 단어로 생성될 확률을 0으로 설정하여 두번 나타나지 않도록 함.\n",
        "\n",
        "예를 들어, `no_repeat_ngram_size=2`을 설정한다면 2-gram이 두번 나타나는 것을 막을 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3iVJgfnkMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48627c9b-9f48-4a5a-e132-9688f8f54ef9"
      },
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신은 조선 중기의 무신이다.</s><s> 그 후, 그는 《삼국유사》(三國遺)의 편찬에 참여하였고, 《동국여지승람(東國地勝)》을 편찬하여 《삼국지\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXTJqkIlFR4L"
      },
      "source": [
        "설정 결과, 더이상 반복이 나타나지 않음. \n",
        "\n",
        "이때 2-gram을 사용하게 될 경우 시의 이름이 전체 텍스트에서 한 번만 나타나기 때문에 city New York이 포함되어있는 기사에서는 사용하지 않는 것이 좋음\n",
        "\n",
        "-> n-gram 패널티는 신중하게 사용해야 함\n",
        "\n",
        "그 외에도, Beam search의 또 다른 중요한 특징은 생성된 Top beam을 비교하여 목적에 가장 적합한 Beam을 선택할 수 있다는 것.\n",
        "\n",
        "Transformer에서 num_return_sequences 파라미터를 return 해야 하는 최대 num_beams 보다 작거나 같도록 설정. `num_return_sequences <= num_beams`로 설정된 코드를 확인 가능."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ClO3VphqGp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea21165-c756-4457-ceb7-b242120bf757"
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그는 《동국여지승람》(東國地勝)을 편찬하였고, 《조선왕조실록》(朝鮮王朝實錄)의 편찬에 참여하였다.\n",
            "1: 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그는 《동국여지승람》(東國地勝)을 편찬하였고, 《조선왕조실록》(朝鮮王朝實錄)의 편찬을 도왔다.\n",
            "2: 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그는 《동국여지승람》(東國地勝)을 편찬하였고, 《조선왕조실록》(朝鮮王朝實錄) 편찬에 참여하여 《삼국\n",
            "3: 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그는 《동국여지승람》(東國地勝)을 편찬하였고, 《조선왕조실록》(朝鮮王朝實錄) 편찬에 참여하여 《대동\n",
            "4: 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그는 《동국여지승람》(東國地勝)을 편찬하였고, 《조선왕조실록》(朝鮮王朝實錄) 편찬에 참여하여 《조\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyRYm1OUI0zN"
      },
      "source": [
        "코드 결과를 통해 볼수 있듯이 5개의 Beam hypotheses는 서로 약간 다름\n",
        "\n",
        "최근, 개방형 생성에서는 Beam search가 최선의 선택사항이 아닐수 있는 몇 가지 이유가 제시되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWQ2EWfYRFJZ"
      },
      "source": [
        "### **Sampling**\n",
        "\n",
        "가장 기본적인 형태의 Sampling은 조건부 확률 분포에 따라 다음 단어 $w_t$를 무작위로 선택하는 것.\n",
        "\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "\n",
        "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
        "\n",
        "\n",
        "Sampling을 이용한 언어생성은 더이상 결정론적이지 않음. 단어 \n",
        "$\\text{\"car\"}$ 는 조건부확률 $P(w | \\text{\"The\"})$에서 샘플링 된 후, $P(w | \\text{\"The\"}, \\text{\"car\"})$에서 $\\text{\"drives\"}$를 샘플링 함.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAz4D-Ks0_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e036530-45e7-4d10-eb84-27eb37789517"
      },
      "source": [
        "# `transformers`에서 `do_sample=True`를 설정하고 `top_k=0`을 통해 *Top-K* sampling을 비활성화.\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, # 완전 random sampling\n",
        "    max_length=50, \n",
        "    top_k=0 # w/o top_k 추출\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신은 조선 중기의 무신이다.</s><s> 신성양우조금(新城兩神養)》으로 명칭이 바뀌었다.</s><s> 비슷한 시기에, 원의 작위를 받아 혼인하였다.</s><s> 아들이 원나라에서 홍종 인으로 태어났다.</s><s> 윤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWjxm9otWZgY"
      },
      "source": [
        "결과는 일관성 없는 문장임\n",
        "-> 이것은 sampling word sequences를 할때 모델이 일관성없이 횡설수설하는 문장을 발생시키는 큰 문제. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n",
        "\n",
        "이 때 적용할 수 있는 한가지 방법은 [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max). 의 이른바 `temperature`를 낮추어 분포 $P(w|w_{1:t-1})$를 더 선명하게 만드는 것. \n",
        "높은 확률의 단어의 가능성은 증가시키고 낮은 확률의 단어 가능성은 감소시키는 효과가 있음. \n",
        "\n",
        "temperature를 적용한다면 다음과 같은 그림을 보이게 됨.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
        "\n",
        "step=1의 다음 단어 분포는 더욱 선명해졌기 때문에 단어 $\\text{\"car\"}$를 선택할 확률이 거의 없다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgJredc-0j0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ee63ee-2105-4e1e-fd72-dfadeacaf5f1"
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7 # `temperature=0.7`를 설정하여 라이브러리에서 분포가 어떻게 변화했는지 보기\n",
        "\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신은 조선 중기의 무신이다.</s><s> 즉, \"거기 윤이 없다\"는 것이다.</s><s> 이는 '거기 윤'이 없다는 것이다.</s><s> 따라서, 독자들의 입장에 서 있는 독자들 또한 이 말을 번역해 내\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn_k6Bykf_vg"
      },
      "source": [
        "이제 이상한 n-gram이 적고 출력 문장이 조금 더 일관성 있게 생성. \n",
        "\n",
        "temperature를 적용하면 분포가 덜 랜덤하지만 `temperature` $ \\to 0$,을 설정한다면 temperature가 적용된 sampling은 greedy decoding과 같아지며 이전과 동일한 문제를 겪음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDhbxGrehxlz"
      },
      "source": [
        "### **Top-K Sampling**\n",
        "\n",
        "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) \n",
        "\n",
        "***Top-K*** sampling은 간단하지만 매우 강력한 생플링 방식을 도입했습니다. . *Top-K* sampling에서 가장 가능성 높은 다음 단어는 필터링 되고 확률 질량은 K 다음 단어에만 재분배됩니다. GPT2는 Top-K Sampling방식을 채택했는데, 이것이 Story Gerneration Task에 성공한 이유중 하나입니다.\n",
        "\n",
        "Top-K Sampling을 더 잘 설명하기 위해 위의 예제에서 두 Sampling step에 사용되는 범위를 3단어에서 10단어로 확장합니다.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "\n",
        "K=6을 설정하면 두 Sampling steps에서 Sampling pool을 6개의 단어로 제한합니다. $V_{\\text{top-K}}$로 정의되는 가장 높은 6개의 단어로  sampling pool을 제한합니다.\n",
        "\n",
        "첫 step에서 전체 확률 질량의 2/3인 0.68정도에 해당하는 단어에서 디코딩되지만, 두번째 step에서 거의 모든 확률질량인 0.99에서 디코딩합니다.\n",
        "\n",
        "그럼에도 불구하고 그것이 두번째 sampling step에서 $\\text{\"not\", \"the\", \"small\", \"told\"}$ 와 같은 다소 이상한 후보들을 성공적으로 제거가 가능했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBtDOdD0wx3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c84b6e9-3e80-43b9-c4f1-7d1690d35ea3"
      },
      "source": [
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신은 조선 중기의 무신이다.</s><s> 그러나, 이 책은 근대 이후 우리가 처한 모든 상황(물론)을 포괄하는 사상들-과 그 방법-을 보여주는 고전적인 사고방식-이 아니라, 실제 속에서 실천의 의미를 찾아가는 것이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV6AHjet0Slr"
      },
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "누적확률이 확률 p를 초과하는 최소한의 단어 집합에서 Sample을 추출.\n",
        "\n",
        "그 후 확률 질량이 단어 집합 사이에 재분배. 이 방법은 다음 단어의 확률 분포에 따라 단어 집합의 크기가 동적으로 증가하거나 감소할 수 있음.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "$p=0.92$을 설정할 경우, 상위 p Sample 추출은 $V_{\\text{top-p}}$로 정의된 확률 질량의 $p=92\\%$를 초과할 최소 단어 수를 선택.\n",
        "첫번째 예에서 가장 가능성 높은 9개의 단어 (\"nice\", \"dog\", \"car\" ...  house)가 포함된 반면, 두번째 예에서는 상위 3개의 단어(\"drives\", \"is\", \"turns\")만 선택해도 92%를 초과하게 됨. \n",
        "즉 높은 확률의 단어에만 Sampling 하고 그렇지 않은 단어는 Sampling할 확률이 매우 적게 됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894AH8E03pv7"
      },
      "source": [
        "Top-p는 또한 Top-K와 함께 사용될 수 있는데, 이것은 매우 낮은 순위의 단어를 피하면서도 일부 동적 선택을 허용함. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kY8P9VG8Gi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43803486-2a63-4fbc-af21-2623078ad5df"
      },
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=20, \n",
        "    top_p=0.92, \n",
        "    num_return_sequences=3 # 독립적으로 샘플링된 다중 출력을 얻기 위하여 파라미터를 다시 설정. `num_return_sequences > 1`: \n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: 이순신은 조선 중기의 무신이다.</s><s> 이 전 대통령측도 같은 이유로 지난 2009년 3월 27일 사저로 이 전 대통령이 오셨을 당시와 지난 2009년 11월 28일 이 전 대통령으로부터 받은 사저에 대한 계약서\n",
            "1: 이순신은 조선 중기의 무신이다.</s><s> 이 과정에서 \"김씨가 자신의 재산을 노리고 있었다\"는 의혹이 불거져 김 전 차관 측은 사실무근이라고 반박했습니다.</s><s> 김 전 차관이 김 전 차관에게 돈을 건넸다는 의혹에 대해 당시 수사팀은\n",
            "2: 이순신은 조선 중기의 무신이다.</s><s> 그는 또한, 자신의 첫 번째 결혼 상대로 장가오리에게 \"사랑해\"라는 짧은 편지를 남겼다.</s><s> 그리고 결혼하여 두 아이를 낳았다.</s><s> 하지만 그 후, 그가 죽은 후, 그는\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4CYi91h11yd"
      },
      "source": [
        "### **Appendix**\n",
        "\n",
        "There are a couple of additional parameters for the `generate` method that were not mentioned above. We will explain them here briefly!\n",
        "\n",
        "- `min_length` can be used to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n",
        "- `repetition_penalty` can be used to penalize words that were already generated or belong to the context. It was first introduced by [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) and is also used in the training objective in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, *e.g.* see this [discussion](https://github.com/huggingface/transformers/pull/2303) on Github.\n",
        "\n",
        "- `attention_mask` can be used to mask padded tokens\n",
        "- `pad_token_id`, `bos_token_id`, `eos_token_id`: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n",
        "\n",
        "For more information please also look into the `generate` function [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
      ]
    }
  ]
}