{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7fa5415",
   "metadata": {},
   "source": [
    "# 1. environment setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e8fa7",
   "metadata": {},
   "source": [
    "device = torch.device('cpu')로 설정시\n",
    " -> to.device 모두 없애기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836da45",
   "metadata": {},
   "source": [
    "## 1.1 argparse\n",
    "https://docs.python.org/3/library/argparse.html?highlight=argparse#type\n",
    "\n",
    "* 1. argumentparser에 원하는 description을 입력하여 parser 객체를 생성한다\n",
    "* 2. add_argument()로 argument는 원하는 만큼 추가가 가능하다\n",
    "* 3. parse_args()로 주어진 인자를 파싱한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b17451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=8, data_dir='', dropout=0.3, early_stop=10, embed_dim=768, epochs=50, expt_type=5, hidden_dim=512, learning_rate=0.01, model_type='lstm+att', num_layers=1, num_runs=50, scale=1.8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse \n",
    "\n",
    "model_types = ('avg-pool', 'lstm+att', 'lstm')\n",
    "\n",
    "experiment_type = (4, 5)\n",
    "\n",
    "parser = argparse.ArgumentParser(\"main.py\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument(\"--expt-type\", type=int, default=5, choices=experiment_type,\n",
    "                        help=\"expt type\")\n",
    "\n",
    "parser.add_argument(\"--batch-size\", type=int, default=8,\n",
    "                        help=\"batch size\")\n",
    "\n",
    "parser.add_argument(\"--epochs\", type=int, default=50,\n",
    "                        help=\"number of epochs\")\n",
    "\n",
    "parser.add_argument(\"--num-runs\", type=int, default=50,\n",
    "                        help=\"number of runs\")\n",
    "\n",
    "parser.add_argument(\"--early-stop\", type=int, default=10,\n",
    "                        help=\"early stop limit\")\n",
    "\n",
    "parser.add_argument(\"--hidden-dim\", type=int, default=512,\n",
    "                        help=\"hidden dimensions\")\n",
    "\n",
    "parser.add_argument(\"--embed-dim\", type=int, default=768,\n",
    "                        help=\"embedding dimensions\")\n",
    "\n",
    "parser.add_argument(\"--num-layers\", type=int, default=1,\n",
    "                        help=\"number of layers\")\n",
    "\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.3,\n",
    "                        help=\"dropout probablity\")\n",
    "\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=0.01,\n",
    "                        help=\"learning rate\")\n",
    "\n",
    "parser.add_argument(\"--scale\", type=float, default=1.8,\n",
    "                        help=\"scale factor alpha\")\n",
    "\n",
    "parser.add_argument(\"--data-dir\", type=str, default=\"\",\n",
    "                        help=\"directory for data\")\n",
    "\n",
    "parser.add_argument(\"--model-type\", type=str, default=\"lstm+att\",\n",
    "                        choices=model_types, help=\"type of model\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383ce52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expt_type': 5,\n",
       " 'batch_size': 8,\n",
       " 'epochs': 50,\n",
       " 'num_runs': 50,\n",
       " 'early_stop': 10,\n",
       " 'hidden_dim': 512,\n",
       " 'embed_dim': 768,\n",
       " 'num_layers': 1,\n",
       " 'dropout': 0.3,\n",
       " 'learning_rate': 0.01,\n",
       " 'scale': 1.8,\n",
       " 'data_dir': '',\n",
       " 'model_type': 'lstm+att'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = args.__dict__\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8032a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_type = config['expt_type']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "epochs = config['epochs']\n",
    "\n",
    "hidden_dim = config['hidden_dim']\n",
    "embedding_dim = config['embed_dim']\n",
    "\n",
    "num_layers = config['num_layers']\n",
    "dropout = config['dropout']\n",
    "dist_values = [15, 21, 42, 13, 9]\n",
    "learning_rate = config['learning_rate']\n",
    "scale = config['scale']\n",
    "\n",
    "loss_type = \"OE\"\n",
    "model_type = config['model_type']\n",
    "\n",
    "number_of_runs = config['num_runs']\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "data_dir = config['data_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d567a1",
   "metadata": {},
   "source": [
    "# 2. dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c5441",
   "metadata": {},
   "source": [
    "dataset: reddit posts of 500 users across 9 mental health and suicide related subreddits.\n",
    "\n",
    "해당 열로 구성\n",
    "   1. label : 0, 1, ... 4 denoting the risk level of the user.\n",
    "   2. enc : list of lists consisting of 768-dimensional encoding for each post. (SISMO uses longformer embeddings [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a996ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_31(five_class):\n",
    "    if five_class!=0:\n",
    "        five_class=five_class-1\n",
    "    return five_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_name = os.path.join(args.data_dir, f'reddit-longformer.pkl')\n",
    "\n",
    "with open(data_name, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "    if expt_type == 4:\n",
    "        df['label'] = df['label'].apply(make_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d567d8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "      <th>label</th>\n",
       "      <th>enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user-0</td>\n",
       "      <td>[Its not a viable option, and youll be leaving...</td>\n",
       "      <td>Supportive</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0849887803196907, 0.08479535579681396, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user-1</td>\n",
       "      <td>[It can be hard to appreciate the notion that ...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2</td>\n",
       "      <td>[[-0.0595586821436882, 0.13912272453308105, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user-2</td>\n",
       "      <td>[Hi, so last night i was sitting on the ledge ...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>3</td>\n",
       "      <td>[[-0.06634160876274109, 0.07215045392513275, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user-3</td>\n",
       "      <td>[I tried to kill my self once and failed badly...</td>\n",
       "      <td>Attempt</td>\n",
       "      <td>4</td>\n",
       "      <td>[[-0.06500397622585297, 0.041644491255283356, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user-4</td>\n",
       "      <td>[Hi NEM3030. What sorts of things do you enjoy...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>2</td>\n",
       "      <td>[[-0.06407008320093155, 0.11472669243812561, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                               Post       Label  \\\n",
       "0  user-0  [Its not a viable option, and youll be leaving...  Supportive   \n",
       "1  user-1  [It can be hard to appreciate the notion that ...    Ideation   \n",
       "2  user-2  [Hi, so last night i was sitting on the ledge ...    Behavior   \n",
       "3  user-3  [I tried to kill my self once and failed badly...     Attempt   \n",
       "4  user-4  [Hi NEM3030. What sorts of things do you enjoy...    Ideation   \n",
       "\n",
       "   label                                                enc  \n",
       "0      0  [[-0.0849887803196907, 0.08479535579681396, 0....  \n",
       "1      2  [[-0.0595586821436882, 0.13912272453308105, 0....  \n",
       "2      3  [[-0.06634160876274109, 0.07215045392513275, 0...  \n",
       "3      4  [[-0.06500397622585297, 0.041644491255283356, ...  \n",
       "4      2  [[-0.06407008320093155, 0.11472669243812561, 0...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafe0da",
   "metadata": {},
   "source": [
    "### enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6fa150",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-8.49887803e-02,  8.47953558e-02,  6.85457736e-02, -1.31821215e-01,\n",
       "         1.32324129e-01, -3.73901278e-02, -2.14210749e-02,  2.54068449e-02,\n",
       "         7.07750991e-02, -8.43656659e-02, -3.06273662e-02,  2.02795002e-03,\n",
       "         3.23151499e-02, -5.38945273e-02,  4.69783731e-02, -6.58048913e-02,\n",
       "        -9.76254418e-02, -4.19864692e-02, -7.71681070e-02, -1.05444938e-01,\n",
       "        -1.21104620e-01, -2.10000314e-02,  5.07857613e-02,  1.53618246e-01,\n",
       "        -4.49450240e-02,  9.10585746e-02,  1.20007150e-01,  1.05090395e-01,\n",
       "         7.29670562e-03,  2.87909620e-02, -4.48305085e-02, -9.90891755e-02,\n",
       "         1.11422956e-01, -1.44968592e-02,  1.28146233e-02,  7.21402168e-02,\n",
       "        -1.08213034e-02,  1.43308295e-02, -4.40211669e-02,  4.20609547e-04,\n",
       "         7.90898651e-02,  2.05816463e-01, -5.76049536e-02, -1.56923775e-02,\n",
       "         5.32514490e-02, -4.63414118e-02, -2.72788703e-02,  6.92395717e-02,\n",
       "        -3.11975218e-02, -6.16822974e-04, -2.16908637e-03,  9.77941304e-02,\n",
       "        -3.83882746e-02,  1.31251020e-02, -1.36670485e-01,  3.55592892e-02,\n",
       "         9.50947255e-02,  5.19960299e-02,  5.79588600e-02, -9.29766372e-02,\n",
       "        -1.99788287e-02, -8.54396448e-02, -6.09583445e-02, -5.62641323e-02,\n",
       "         5.35516553e-02, -6.68325648e-02, -4.95883189e-02,  1.07907597e-02,\n",
       "         5.74005507e-02,  8.11784044e-02,  5.30488007e-02, -2.61227768e-02,\n",
       "         1.03054335e-02,  4.98821214e-02, -3.98627073e-02, -4.31482680e-02,\n",
       "        -5.19572906e-02,  3.72482032e-01, -1.15140580e-01, -1.73122063e-02,\n",
       "         9.17845815e-02, -6.32782280e-02,  3.76957029e-01,  2.68381555e-02,\n",
       "         2.75132693e-02, -1.19411655e-01,  1.31446436e-01,  1.67643353e-02,\n",
       "         5.19781932e-02,  1.32741118e-02,  4.75782640e-02,  5.14693782e-02,\n",
       "        -3.89775969e-02, -5.83382733e-02,  6.52363598e-02, -2.50332970e-02,\n",
       "         1.03497766e-02, -2.64146477e-02, -2.70722713e-02, -1.31253079e-01,\n",
       "        -1.51033206e-02, -9.18547288e-02,  9.86515507e-02,  1.02137595e-01,\n",
       "        -3.75367962e-02,  2.89683342e-02,  3.75820175e-02, -6.43532351e-02,\n",
       "         1.17671326e-01, -8.65139440e-02,  1.95799135e-02, -6.62489980e-02,\n",
       "         7.86198229e-02, -3.39791551e-02, -5.77343861e-04, -7.24752322e-02,\n",
       "        -1.17572509e-02,  3.35349813e-02,  4.63381410e-02,  1.16455927e-01,\n",
       "        -1.78557821e-02,  7.04399198e-02,  1.33665860e-01, -1.05459057e-01,\n",
       "        -1.07710354e-01, -1.52553748e-02, -8.15063640e-02, -2.16604788e-02,\n",
       "         4.39146496e-02,  7.60648586e-03,  9.16045066e-03, -2.24401459e-01,\n",
       "        -2.68867090e-02,  3.72684486e-02, -8.89583491e-03, -4.02251352e-03,\n",
       "        -1.32158687e-02, -6.78493902e-02,  3.85742373e-04, -9.14874487e-03,\n",
       "        -3.01299039e-02,  2.24911142e-02,  4.08319831e-02,  9.54023842e-03,\n",
       "         1.63449869e-01,  7.19266906e-02, -5.01800291e-02, -1.46352192e-02,\n",
       "        -3.63006778e-02, -3.33516076e-02,  7.24974722e-02, -5.22666797e-02,\n",
       "        -1.82420332e-02, -6.52571255e-03, -7.43200853e-02,  5.82056582e-01,\n",
       "         1.37413904e-01,  1.35952950e-01,  6.47265837e-03,  2.77183745e-02,\n",
       "         1.87787637e-01, -2.63625267e-03,  6.89507276e-02,  1.14132660e-02,\n",
       "        -5.69980256e-02,  3.02265882e-02, -7.73335472e-02, -3.30356844e-02,\n",
       "         4.90850061e-02,  1.49351277e-03,  1.38729841e-01,  4.56252508e-02,\n",
       "         6.06970936e-02, -6.67815432e-02, -8.74524489e-02, -6.53505623e-02,\n",
       "         1.12026244e-01, -5.58944931e-03, -1.08080424e-01,  1.82170048e-03,\n",
       "         4.28597517e-02,  8.83678868e-02, -3.91025022e-02, -1.10834641e-02,\n",
       "        -6.29436225e-02, -1.96675980e-03, -3.14679071e-02,  3.97542343e-02,\n",
       "         3.75435725e-02,  6.59392104e-02,  3.41664739e-02,  1.62865280e-03,\n",
       "         5.34485132e-02, -5.53254075e-02,  5.72550781e-02,  1.63280427e-01,\n",
       "         4.44196630e-03, -6.66902512e-02,  1.86361391e-02, -1.05741426e-01,\n",
       "         2.11496707e-02, -3.10102366e-02,  1.62097573e-01, -9.60079581e-02,\n",
       "         6.86316565e-02, -1.84744317e-02,  4.64962125e-02,  5.14688268e-02,\n",
       "         2.66757756e-02, -3.12208291e-02, -6.27196580e-02,  6.90314248e-02,\n",
       "         4.79144268e-02,  1.46500602e-01,  7.69493709e-05,  5.42543642e-03,\n",
       "         6.47889599e-02,  2.11696073e-01,  2.41202768e-02,  3.23294550e-02,\n",
       "         3.55845913e-02,  1.52132697e-02, -2.29869224e-02,  2.13639513e-02,\n",
       "         2.71691363e-02, -2.13218238e-02,  2.96174642e-02,  7.78852776e-02,\n",
       "         3.26933526e-02,  1.33387186e-03, -3.99800055e-02,  1.11308405e-02,\n",
       "        -6.00468926e-03, -1.97480340e-02,  7.21234381e-02, -3.14188339e-02,\n",
       "        -4.53620516e-02, -3.92563408e-03, -7.33273923e-02,  4.62486334e-02,\n",
       "        -5.60484044e-02,  7.40831941e-02,  7.23976269e-02,  8.95793661e-02,\n",
       "         6.37777448e-02,  1.32231817e-01,  7.61386454e-02,  7.07267523e-02,\n",
       "         1.75422411e-02,  4.19909880e-02, -4.00716662e-02, -3.22237052e-02,\n",
       "        -5.48030920e-02, -1.41817452e-02,  5.23885749e-02, -5.86361028e-02,\n",
       "        -1.20180748e-01, -4.36624475e-02, -1.08132511e-01, -1.23092055e-01,\n",
       "        -4.27835323e-02, -4.43117991e-02, -2.00876035e-02, -2.14486141e-02,\n",
       "        -5.35564795e-02, -1.23819344e-01, -1.23699106e-01,  5.47715579e-04,\n",
       "         7.62251765e-02, -3.40092136e-03,  7.73282535e-03, -1.14323653e-01,\n",
       "         4.25966876e-03, -1.39361814e-01,  6.10669628e-02, -2.99973842e-02,\n",
       "        -4.85140085e-02,  5.41653484e-02, -6.02425076e-02,  3.78708132e-02,\n",
       "         2.72549894e-02, -1.26207871e-02, -1.31635815e-01, -2.56704772e-03,\n",
       "         1.56972166e-02,  3.27526890e-02, -7.87863135e-02, -3.82663347e-02,\n",
       "         2.24244222e-02,  8.36845115e-02,  9.21601430e-02,  2.49918103e-02,\n",
       "        -1.17585450e-01,  5.14541082e-02,  5.02234176e-02, -1.09982835e-02,\n",
       "         4.65222448e-02,  5.33143524e-04,  3.75213437e-02, -2.84606721e-02,\n",
       "        -2.27272995e-02, -2.97948271e-02,  2.97256839e-03, -6.58692271e-02,\n",
       "         4.04658653e-02,  1.62037369e-02,  2.18323190e-02, -2.01897286e-02,\n",
       "         4.51134369e-02,  3.36778276e-02,  5.92649973e-04, -8.29382762e-02,\n",
       "        -8.61136988e-02,  9.80307311e-02,  5.91275245e-02,  6.46054372e-03,\n",
       "         6.47628773e-03, -1.06291398e-01,  2.56129652e-02,  9.80030862e-04,\n",
       "         1.98534932e-02,  8.05485323e-02,  7.36850128e-02, -7.77838193e-03,\n",
       "        -3.17041054e-02,  6.35370687e-02,  3.33208293e-02,  1.42675228e-02,\n",
       "         1.95738859e-02,  4.07087833e-01, -3.24156284e-01,  1.38277099e-01,\n",
       "         1.46455005e-01, -2.22054776e-03,  9.34586376e-02, -3.52465734e-02,\n",
       "         8.28470383e-03,  6.21069297e-02,  1.36619508e-01,  1.38420105e-01,\n",
       "        -5.66129796e-02,  2.76924782e-02, -1.08678982e-01,  8.28994513e-02,\n",
       "        -4.97336779e-03,  9.43825208e-03, -1.13560837e-02, -8.59395340e-02,\n",
       "        -2.27687657e-02,  5.60963415e-02,  1.36051085e-02, -6.78735450e-02,\n",
       "         1.11330682e-02, -5.70710860e-02,  1.83610413e-02,  8.23086947e-02,\n",
       "        -8.35548621e-03,  2.68594152e-03,  4.25859988e-02, -2.98458301e-02,\n",
       "         5.36039248e-02, -2.95792930e-02,  6.80159684e-03, -1.16322987e-01,\n",
       "         5.00539280e-02, -4.01477069e-02, -1.43527374e-01,  8.54187161e-02,\n",
       "         4.23267148e-02, -4.20751125e-02,  7.19002858e-02, -5.01979552e-02,\n",
       "        -8.49392544e-03, -5.64340362e-03, -4.24083844e-02,  2.78854999e-03,\n",
       "         2.82310247e-02,  3.38905118e-02,  4.72433679e-02,  1.97155669e-01,\n",
       "        -2.47229449e-02,  8.57096165e-02, -8.41622129e-02,  5.22625633e-02,\n",
       "         1.01585232e-01, -6.06463030e-02,  3.15441862e-02, -5.98633103e-03,\n",
       "         2.91539114e-02,  6.00480288e-02, -5.49558252e-02, -2.89153513e-02,\n",
       "        -3.95070165e-02, -1.34897545e-01,  5.50776385e-02,  1.74974222e-02,\n",
       "        -4.11285162e-02, -1.08033657e-01,  6.31201118e-02, -4.11006398e-02,\n",
       "         1.50636248e-02,  6.32822439e-02, -3.80826704e-02,  5.95028326e-02,\n",
       "         2.55015288e-02, -4.92899977e-02,  2.26002615e-02, -3.61882560e-02,\n",
       "         8.09320435e-03, -1.07826255e-01,  3.04549839e-02,  6.77403584e-02,\n",
       "         7.99541473e-02, -4.74279672e-02,  8.06751475e-02, -7.10586831e-02,\n",
       "         2.75184698e-02, -8.22528675e-02, -7.48698832e-03, -2.08728015e-02,\n",
       "        -1.25825414e-02,  7.10070208e-02,  4.05261070e-02, -8.20506662e-02,\n",
       "         8.32393318e-02,  2.51431763e-02,  1.89903248e-02, -1.24172434e-01,\n",
       "        -9.35492665e-02, -1.32435397e-03, -8.84258002e-02,  6.06650114e-02,\n",
       "        -9.76661891e-02, -1.45397549e-02, -1.10985227e-02, -6.69515356e-02,\n",
       "        -3.05096395e-02,  3.63233164e-02, -4.59762439e-02, -1.28534362e-01,\n",
       "        -3.12750936e-02, -1.99644752e-02,  3.27208042e-02,  3.81602272e-02,\n",
       "         2.79050972e-02, -1.72704086e-02,  7.83813745e-02,  3.31375226e-02,\n",
       "         3.20386025e-03,  1.76144261e-02, -4.30288613e-02, -1.14946673e-02,\n",
       "        -7.90119022e-02, -3.81524026e-01,  3.21073011e-02, -1.29284558e-03,\n",
       "         5.17326444e-02,  2.71423031e-02, -3.14630456e-02,  1.84861235e-02,\n",
       "        -2.78650466e-02, -9.68131647e-02,  2.42161807e-02, -7.38929212e-02,\n",
       "         3.92753771e-03, -5.68218641e-02, -1.07134625e-01, -3.89746465e-02,\n",
       "        -4.66912985e-02, -6.78213388e-02,  1.06561400e-01, -1.32233566e-02,\n",
       "        -1.84749514e-02, -1.74815267e-01,  8.24073702e-02,  4.87842038e-03,\n",
       "         2.28872485e-02, -7.63770789e-02, -4.13220003e-02, -9.77805108e-02,\n",
       "        -7.79514685e-02,  5.99932335e-02,  6.75165877e-02,  2.80446056e-02,\n",
       "        -5.14722317e-02,  5.58456779e-03, -5.40785715e-02,  1.22606698e-02,\n",
       "         1.60006732e-01,  1.48558794e-02, -7.79866334e-03, -5.09543680e-02,\n",
       "         8.85371938e-02,  6.68908358e-02,  1.80606365e-01, -8.26051831e-02,\n",
       "         6.59256279e-02, -6.15033787e-03,  9.22337249e-02,  5.05125038e-02,\n",
       "        -1.79990113e-03, -2.87183877e-02,  7.74527609e-04, -6.63680816e-03,\n",
       "        -4.51528095e-02,  2.30397917e-02, -3.34271714e-02, -4.79762144e-02,\n",
       "        -3.22017074e-02, -2.69923136e-02, -8.55081342e-03,  1.12933628e-02,\n",
       "         2.21865237e-01,  2.85728592e-02,  3.51628438e-02, -4.97567803e-02,\n",
       "        -7.87863582e-02,  9.84078366e-03, -1.04875363e-01, -5.38826883e-02,\n",
       "        -6.67513609e-02,  1.06812017e-02, -7.26455403e-03, -5.95499091e-02,\n",
       "        -6.94315210e-02, -4.45934199e-02, -5.01542725e-03, -6.83603659e-02,\n",
       "         1.11285977e-01,  3.31825204e-02,  5.58583699e-02, -5.38356602e-02,\n",
       "         8.69467705e-02,  6.84859753e-02,  2.46283086e-03,  2.13011801e-02,\n",
       "         1.68374851e-01,  2.01570429e-02,  1.42875407e-02, -2.40662601e-02,\n",
       "        -7.78420344e-02,  5.88230938e-02,  1.44425765e-01, -5.49551994e-02,\n",
       "         4.43614535e-02,  6.76483512e-02,  3.32436413e-02, -5.46459854e-02,\n",
       "         9.82584804e-02, -1.11792106e-02,  6.05213009e-02, -5.32058299e-01,\n",
       "        -2.82480419e-02,  1.34164900e-01, -1.95365194e-02, -2.91424748e-02,\n",
       "         1.07852444e-01,  7.59309605e-02, -4.05667536e-02, -1.56535786e-02,\n",
       "        -4.08864468e-02,  1.52681977e-01,  2.96221673e-02,  6.95587471e-02,\n",
       "        -6.70140386e-02, -5.93899563e-02,  3.32091115e-02,  1.99706061e-03,\n",
       "         4.59489645e-03,  4.01964933e-02, -2.08026916e-01,  4.43719178e-02,\n",
       "        -6.23570196e-02,  1.78155094e-01,  6.02472685e-02,  4.72384952e-02,\n",
       "         7.41735250e-02, -7.63248727e-02,  6.13973774e-02,  3.21695395e-02,\n",
       "        -1.02881538e-02, -9.11136065e-03, -2.73908861e-02, -4.63618860e-02,\n",
       "         7.81001672e-02,  5.53180173e-04,  8.87761712e-02, -4.48155263e-03,\n",
       "         1.23331308e+01,  1.34703135e-02,  4.77758348e-02, -1.14648513e-01,\n",
       "         4.84173223e-02, -1.24816895e-01,  1.52996043e-02, -2.14950740e-01,\n",
       "        -1.86633561e-02,  1.27268359e-01, -2.21238527e-02, -1.05379663e-01,\n",
       "        -5.26429377e-02, -1.14868991e-01,  1.31160682e-02, -2.87424624e-02,\n",
       "        -3.28867808e-02,  8.10880512e-02,  1.06243733e-02, -4.75107916e-02,\n",
       "        -3.00726723e-02,  1.05860233e-01,  7.53941536e-02, -4.08371389e-02,\n",
       "        -1.43939173e-02,  1.16415448e-01,  8.44412819e-02, -7.78976604e-02,\n",
       "        -2.74711801e-03,  3.56489271e-02, -3.42670046e-02, -1.03896745e-02,\n",
       "         6.54758653e-04, -5.55309728e-02,  9.11476389e-02,  8.53057727e-02,\n",
       "         9.66670066e-02,  1.37262210e-01,  6.77983537e-02,  1.49065312e-02,\n",
       "         1.72502771e-02,  1.89295840e-02,  7.67719001e-02,  4.09861542e-02,\n",
       "         5.61705790e-02,  2.99557503e-02,  5.72474711e-02,  1.87335871e-02,\n",
       "        -1.79179702e-02,  8.32724851e-03,  1.07484639e-01, -1.43805286e-03,\n",
       "         1.23701118e-01, -1.15079014e-02, -3.91118787e-02,  5.77434078e-02,\n",
       "        -6.67062961e-03, -1.04911946e-01,  8.81529450e-02,  9.00828689e-02,\n",
       "        -9.16063488e-02,  1.45324051e-01,  3.17125022e-02,  1.32491708e-01,\n",
       "         7.17635900e-02,  1.36703163e-01,  8.66998509e-02,  6.74790964e-02,\n",
       "        -9.73435789e-02,  7.15239439e-03, -3.48690674e-02, -1.28121555e-01,\n",
       "        -5.81092052e-02,  8.55384395e-02,  7.94645995e-02, -3.38947698e-02,\n",
       "         4.84674647e-02,  2.29754625e-03,  1.64230168e-02, -1.44811701e-02,\n",
       "         3.33932415e-03,  2.75318641e-02,  2.11330391e-02,  7.16819894e-03,\n",
       "         1.23312086e-01,  1.12071317e-02,  5.95423840e-02,  1.24524543e-02,\n",
       "        -2.11764406e-02, -5.01113981e-02, -3.72295342e-02,  1.50863584e-02,\n",
       "         1.13166953e-02, -8.05631652e-02, -2.75658648e-02, -8.97093341e-02,\n",
       "         4.07608151e-02, -1.67661950e-01,  1.04775012e-01,  1.51380062e-01,\n",
       "        -6.46927953e-02,  4.03466113e-02, -5.45296120e-03,  9.67251956e-02,\n",
       "         5.05974405e-02,  3.18391770e-02, -4.47494797e-02, -1.13441199e-02,\n",
       "        -8.73068627e-03,  2.77292263e-02, -1.05423364e-03,  5.97582236e-02,\n",
       "         1.07382998e-01, -3.77748162e-02,  8.26738477e-02,  3.88115570e-02,\n",
       "         3.60596776e-02, -3.48290503e-02,  6.90489784e-02,  5.53392433e-02,\n",
       "        -6.15179278e-02,  4.24988158e-02, -1.18368845e-02, -1.17576487e-01,\n",
       "        -7.42669180e-02, -6.72819987e-02, -1.42188910e-02,  3.13260630e-02,\n",
       "        -1.34909987e-01, -5.10055665e-03,  4.69260812e-02,  2.65475661e-02,\n",
       "         3.11665535e-02, -1.06883422e-01,  1.02200672e-01, -1.13610879e-01,\n",
       "        -1.19477957e-01,  4.29525226e-02,  1.73935294e-02,  5.24590537e-02,\n",
       "         1.52435191e-02, -4.65515908e-03, -6.27570525e-02, -1.28576890e-01,\n",
       "        -3.67825292e-02,  1.82946250e-01,  3.18127088e-02,  6.37174845e-02,\n",
       "         5.50514311e-02, -8.53798911e-02, -2.34274510e-02,  6.40236139e-02,\n",
       "         1.25854835e-01,  4.58586328e-02, -5.38559770e-03, -3.58068794e-02,\n",
       "        -1.17710838e-03,  1.23045757e-01, -3.39699276e-02, -3.51011083e-02,\n",
       "        -1.61259007e-02, -7.96990916e-02,  4.29983772e-02,  9.94642526e-02,\n",
       "         2.29707472e-02,  1.78069044e-02,  6.91598048e-03,  9.22116917e-03,\n",
       "         6.82880823e-03, -3.24378908e-02,  3.11905760e-02,  8.23573489e-03,\n",
       "        -7.35591576e-02, -4.05613743e-02,  1.76168419e-02,  9.28747579e-02,\n",
       "         6.68860450e-02, -7.37094134e-02, -6.22881576e-03, -2.83861067e-02])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc0d6d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.enc[1][0]) # 768 # 각 리스트의 차원은 768로 같다\n",
    "len(df.enc[1][1]) # 768\n",
    "\n",
    "len(df.enc[3]) # 4- post에 따라 숫자가 다르다\n",
    "len(df.enc[1]) # 8\n",
    "len(df.enc[0]) # 1\n",
    "# -> 문단의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfa5f4",
   "metadata": {},
   "source": [
    "### Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f2315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Its not a viable option, and youll be leaving your wife behind. Youd Pain her beyond comprehension.It sucks worrying about money, I know that first hand. It can definitely feel hopeless, as you seem to be Tired aware of. Your wife might need to chip in financially. I know time is an issue, but even 10-15 hours a Asthenia could alleviate a lot of the pressure. In the meantime, get your shit together - write that resume tomorrow. No excuses, get it done and send it out. Whether you believe in some sort of powerful being or force governing things or not, things really do work themselves out. This is a big test for you, and youll pull through. Just try to stay as positive as you can and everything will work out.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df.Post[0]))\n",
    "df.Post[0] # 문단이 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9201552",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It can be hard to appreciate the notion that you could meet someone else who will make you happy when you are so deeply in love with your boyfriend. Your desires are set on him and not much else will make you happy at the moment. But over time the mind has a way of dealing with loss, this is a proven fact in psychology. Over time, one day you will arrive at a level where you again feel at peace and can start feeling comfortable at looking into new relationships.It is certainly uncomfortable dealing with your current situation but do understand that with some time and patience the Pain will go away and you will get through the difficulties that many US students are encountering with debt, unemployment and undervalued degrees.These are problems that many of us are facing right now, you are not alone. It is crushing to face all these difficulties at the same time, but getting through them is what is going to make us stronger, smarter and more emotionally aware than any generation that came before us. ',\n",
       " 'The voice is just a voice. People can praise you, people can hate you but it doesnt change who you are. Think about it. People can come up to you and say you are awesome, but does that change you in any way? No. It is a psychological thing.Think about how absurd it is what you are saying here. A little voice is telling you to kill yourself, so youre actually going to do it?If I went up to a random person and told them to kill themselves, they would probably walk away from me.You should persevere. You do so by not identifying with it. It might always be there, negative feelings and thoughts always present themselves to us when we are vulnerable. But a voice is just a voice, it is not us, and we only suffer under its rule when we allow it an ear.This happens to everyone, it is just a matter of depth. Suicidal people have invested a lot of energy into \"I\" thoughts, such as \"I am worthless, I am miserable\". And you breathe life right into them by doing that, and they come back to sap more life. Let them starve!Any insult against you is worthless unless you give it credit. It is like a cheque that bounces at the bank.So dont give into your negative thoughts and feelings. Stop giving them credit. This is hard in the beginning, and hard over time, but you know you can actually be content and live a decent life if you take this seriously. Stop giving the \"I\" so much credit.',\n",
       " 'You indeed sound Tired bipolar. Bipolar is when you take things to such extremes. It is a terrible illness. I suffered it myself at one point, and have taken lithium and a variety of other medications to deal with it. Its not so bad. It helps. But ultimately the problem is a psychological one. Youre going to have to feel your way around your body and understand your own psychological profile, your triggers and so forth.What is also Tired important to note that bipolarity is a two way street. You might have these extreme moments of aversion (the knife carving incident), but what people dont realize is that you also have your moment of desires, or what I mean by that is, you go way too far into the things you love.If you were just a bit more indifferent about things youd stabilize a great deal, I think, having had the illness. Obviously stop taking drugs and stuff like that if youre taking them, but also stop staying up so late, or doing whatever it is you do to an extreme. Live a balanced life. Bonus points if you find that life boring. Acclimate to that and make it your new norm, a stable life is a happy life. Live a stable life where you are relatively indifferent to most things, you will enjoy your accumulated efforts over the years and you will be pleased. Do NOT go into extremes of Pain and pleasure, which is essentially what you are doing. Leave well enough alone.',\n",
       " 'You will always come back so much stronger afterwards. Ive been cheated on in at least one or two long term relationships, sometimes I dont know if the girl was faithful in one of the relationships, I digress... it is extremely painful, you will suffer for months on end but when you come out of it you will be better than you were before, hardened by fire. Then you will meet somebody worth your time. The person Im going out with now is the best person Ive ever gone out with by far... the experience will prove that you are worth someone who is loyal and awesome.',\n",
       " 'You should never allow yourself the space to feel those feelings. They are not some sort of natural feeling as people say they are. Any sort of feeling that started back when you were a teenager aught to be called into question. We all have many stupid feelings as a teenager. The problem is when those follow you into adulthood.I know people who are over 26 years old and engaging in the exact same habits that they were since they were 13. I mean thats just sad. When are they going to grow up and move on? Why do people get stuck with such old sufferings?Life is Tired hard, it is a constant improv but there are different ways of looking at it. Not everything is \"bad\". Just because youre suffering doesnt mean you need to take it so deeply to heart. I mean come on, you have a family that loves you. I avoid a lot of problems because I try not to focus on myself so much. I think of what I can do for others. This might sound egotistical in itself but really you have to understand the notion behind it. When we think with our mind, we always become Tired miserable. We never become happy through the way we think. Nothing is ever perfect or good when our mind has a say in it.But when youre trying to help others, its through the heart. And it feels good to help them. It makes one happy.I always recommend in these difficult times that people stop placing so much importance on their mind. Never allow negative thoughts inside your mind. Protect yourself with positive emotion. If robbers were pounding on your door, would you open the door to them? No. But then Suicidal thoughts come, and they are just as bad as murderers and those who want to harm your wife and family. Why do you allow them space?It is Tired important for you to manage your mind. If you have difficultly conjuring positivity in your interior world, then you need to step back and question why that is? What is holding you back from being a happy person?Quite often you will find it is these \"old friends\", these thoughts you identify with so much and feel to be so real. Its almost like they understand you.Well you made them with your thoughts, they subsist on your energy that they gain from you eating your food. Its entirely personalized.That is why it is so difficult to stop identifying with the thoughts. But you have to realize, they are coming to you, so theyre not you. Theyre just thoughts. Youre in control. You dont need to feed into that destructive cycle. Force the negative thoughts out, constantly demand that they stay out. Cultivate positivity, love your wife and child, do things to make them happy. Absorb that in.Positivity drives the negative thoughts out. Negative thoughts when accepted destroy the positivity. Its a real dragon and tiger scenario, but you need to stick on the good side. Never give the enemy an inch. Theyll take a mile. Retire your old way of thinking. It is a failed way. It has done you no good. Open your mind up, embrace positivity. Have a revolution, insist on embracing life. Well all die, sooner or later but lets have a part in bringing happiness to this planet while we can. We need people like you to help us, to come and help others. You are strong, youve made it this far. But you made a mistake, you leave the door open to that old enemy. Close it, close it every time and bar it off. You are not that thought of suicide. ',\n",
       " 'I was like you once, I was miserable and broken. Almost ended up with my life completely ruined, I lived way too dangerously and almost lost my freedom, ex long term girlfriend and everything I loved. I turned my life around, picked up the pieces and built a new life out of it. Now my life is exciting, I am talking to a beautiful new gal, am progressing well in my studies and everything is looking up. Do not identify with the down times in your life. It always seems like it will never get better but thats not true. Eventually good things do happen, but we all have to do our part too.',\n",
       " 'Dont be so hard on yourself man. Ive lived on my own for only like five months and Im 27. It is extremely expensive to do that sort of thing nowadays. Back in the day, you used to be able to build a house out of logs and a guy your age could have 6 kids already. Nowadays many are lucky to buy a house by the time they are 50.Dont get all tied up in with this society. It will burn you out. It doesnt care about who you are inside, it just cares with what you can do. Mechanical labor.Be OK with taking some time for yourself. Stop dating for a while. Be OK with taking some time to relax. You are too wound up. You arent going to find happiness by \"doing things\". No matter how big the check list or how many things you check off, that does not necessarily mean you will have lived a Tired successful life.Instead, take some time to get to know yourself. When you lay down to have a nap, dont completely go to sleep, just rock back and forth a little bit to barely stay away, and just feel yourself in the bed, just be aware you are there and keep doing that over and over again. Feelings will come to you and thoughts will come to you but you will become more aware of them. Just really feel them and get to understand them.You can change how everything is. The reason you have all these girls cheat on you is because you cannot see them for how they are. Youre not able to see their real values, youre not able to see who they are by just looking into their eyes.Take some time and read some philosophy from ancient Greece, appreciate some fine art, read Wikipedia.Its OK to take time. If you just go out there again like a wild animal, you will be in a wild chaotic abyss of angst, Fear and listlessness. ',\n",
       " 'Just be. Look at how you are thinking here:\"Ive never been good at anything. I suck at school, sports, art..everything. I have considered suicide on and off pretty much since I was 13(Im 20 now). I never had friends Tired long, they always decided they didnt like me and stopped talking to me.\"Each of these thoughts is self enforcing. They were created by you and fed by you. And they will cease to exist when you stop feeding into them. It is a negative thought cycle.No self respecting person accepts negative thoughts, negative emotions or negative people. Try to see the good in everything, to watch nice things and be around people who give you positive feelings.If your boyfriend is just sticking around for the money... well then, that doesnt sound Tired positive, does it? Only you can know if he contributes positive value to your life.But how foolish it is to assume that by going through the same psychological cycles that things would improve. Life works in cycles. Everyones goes to work, goes home and watches TV. That is hardly unusual nowadays.The difference is, people can deal with that because they dont think negative thoughts about themselves. You have to close the doors to all negativity, it is up to you. When you open the doors to negative emotion, then it will sap you greatly.Spend some time around your mother. See the positive in your relationship with her. She can teach you a lot. But you have to be willing to learn.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df.Post[1]))\n",
    "df.Post[1] # 문단이 8개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd2fa5",
   "metadata": {},
   "source": [
    "## data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c502506",
   "metadata": {},
   "source": [
    "#### train_test_split\n",
    "* train_test_split(data, target, test_size=0.25, shuffle=True, stratify=target, random_state=34)\n",
    "\n",
    "         * stratify: default=None, \n",
    "                classification에서 중요한 옵션값으로 target으로 지정시 각각의 class 비율(ratio)을 train / validation에 유지해줌. 즉, 한 쪽에 쏠려서 분배되는 것을 방지해주는 역할."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65d07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, _, __ = train_test_split(\n",
    "            df, df['label'].tolist(), test_size=0.2, stratify=df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2637fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.size) # 400 * 5 = 2000, 전체 데이터 개수가 500개인데 test_size=0.2로 지정했기에 train은 400개\n",
    "df_test.size # 100 * 5 = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c9d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User', 'Post', 'Label', 'label', 'enc'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0529553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 4, 2, 3, 2, 2, 4, 2, 1, 1, 2, 2, 0, 2, 3, 0, 4, 2, 2, 2,\n",
       "       2, 0, 3, 4, 3, 2, 0, 2, 1, 0, 2, 4, 1, 4, 0, 0, 3, 2, 0, 0, 0, 0,\n",
       "       0, 3, 0, 0, 0, 3, 0, 2, 2, 0, 2, 3, 3, 4, 3, 4, 3, 2, 0, 4, 1, 2,\n",
       "       2, 0, 2, 2, 1, 0, 3, 0, 2, 3, 2, 1, 3, 1, 2, 3, 3, 2, 0, 2, 0, 2,\n",
       "       4, 0, 4, 0, 0, 0, 2, 0, 3, 2, 0, 0, 3, 0, 3, 2, 1, 2, 3, 2, 3, 4,\n",
       "       4, 1, 0, 3, 0, 0, 2, 2, 2, 0, 2, 1, 1, 1, 0, 0, 0, 3, 3, 1, 0, 0,\n",
       "       2, 0, 0, 2, 1, 0, 2, 3, 1, 1, 2, 2, 0, 2, 2, 3, 4, 0, 3, 0, 2, 4,\n",
       "       1, 0, 1, 1, 1, 2, 0, 3, 2, 0, 0, 1, 0, 2, 4, 1, 2, 2, 1, 4, 2, 1,\n",
       "       4, 2, 1, 1, 2, 2, 2, 2, 0, 1, 2, 2, 2, 1, 2, 0, 0, 0, 0, 3, 1, 2,\n",
       "       3, 3, 3, 1, 1, 2, 1, 0, 0, 2, 1, 3, 3, 1, 0, 2, 0, 3, 3, 1, 3, 0,\n",
       "       2, 2, 1, 0, 4, 2, 3, 2, 3, 1, 2, 2, 3, 2, 1, 3, 1, 2, 4, 3, 1, 0,\n",
       "       3, 1, 2, 1, 2, 2, 3, 2, 0, 3, 0, 2, 2, 0, 4, 1, 2, 3, 2, 1, 2, 1,\n",
       "       4, 2, 4, 1, 0, 2, 2, 0, 2, 2, 4, 3, 2, 0, 1, 0, 1, 0, 4, 2, 2, 2,\n",
       "       2, 3, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 3, 4, 2, 1, 2, 1, 2,\n",
       "       3, 3, 3, 2, 0, 1, 2, 1, 2, 2, 4, 2, 3, 1, 3, 1, 0, 1, 2, 4, 1, 3,\n",
       "       2, 2, 3, 2, 2, 2, 3, 4, 1, 4, 0, 0, 2, 4, 2, 1, 3, 1, 3, 2, 1, 0,\n",
       "       2, 4, 1, 2, 0, 4, 2, 2, 3, 1, 3, 2, 2, 4, 0, 2, 1, 2, 2, 1, 3, 0,\n",
       "       2, 2, 1, 2, 4, 1, 1, 2, 2, 0, 2, 0, 1, 0, 2, 1, 2, 4, 2, 2, 1, 2,\n",
       "       1, 0, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.label.values))\n",
    "df_train.label.values # 라벨 값은 0-4로 구성(C-SSRS기반, 5가지 분류: Support (SU) < Indicator(IN) < Ideation (ID) < Behavior (BR) < Attempt (AT)\n",
    "# df_train.enc.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e1139",
   "metadata": {},
   "source": [
    "# 3. 끝까지 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f42ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (AdamW, get_cosine_schedule_with_warmup,\n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup)\n",
    "import optuna\n",
    "\n",
    "from dataset import RedditDataset\n",
    "from loss import loss_function, true_metric_loss\n",
    "from model import RedditModel\n",
    "from utils import class_FScore, gr_metrics, make_31, pad_collate_reddit, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364386c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RedditDataset(\n",
    "    df_train.label.values, df_train.enc.values)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=pad_collate_reddit, shuffle=True)\n",
    "\n",
    "test_dataset = RedditDataset(df_test.label.values, df_test.enc.values)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, collate_fn=pad_collate_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b471804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model_type'] == 'lstm+att'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16448ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RedditModel(expt_type, embedding_dim,\n",
    "                                hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33d22afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedditModel(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc_2): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  (historic_model): MyLSTM(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm1): LSTM(768, 512, bidirectional=True)\n",
       "    (atten1): Attention()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b7bad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),\n",
    "                 lr=learning_rate)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "optimizer, num_warmup_steps=10, num_training_steps=epochs)\n",
    "\n",
    "best_metric = 0.0\n",
    "tc = time.time()\n",
    "\n",
    "early_stop_counter = 0\n",
    "early_stop_limit = config['early_stop']\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7dc5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, expt_type, dataloader, optimizer, device, dataset_len, loss_type, scale=1):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for bi, inputs in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels, tweet_features, lens = inputs\n",
    "\n",
    "        labels = labels#.to(device)\n",
    "        tweet_features = tweet_features#.to(device)\n",
    "\n",
    "        output = model(tweet_features, lens, labels)\n",
    "\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        loss = loss_function(output, labels, loss_type, expt_type, scale)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects.double() / dataset_len\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "109d7ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsail/hyolim/sismo-wsdm/dataset.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  tweets = torch.tensor(self.tweets[item], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#for epoch in trange(epochs, leave=False):\n",
    "loss, accuracy = train_loop(model,\n",
    "                            expt_type,\n",
    "                            train_dataloader,\n",
    "                            optimizer,\n",
    "                            device,\n",
    "                            len(train_dataset),\n",
    "                            loss_type,\n",
    "                            scale)\n",
    "if scheduler is not None:\n",
    "    scheduler.step()\n",
    "\n",
    "if loss >= best_loss:\n",
    "    early_stop_counter += 1\n",
    "else:\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    early_stop_counter = 0\n",
    "    best_loss = loss\n",
    "\n",
    "#if early_stop_counter == early_stop_limit:\n",
    " #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4adae808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, expt_type, dataloader, device, dataset_len, loss_type, scale=1):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "\n",
    "    fin_conf = []\n",
    "\n",
    "    for bi, inputs in enumerate(dataloader):\n",
    "        labels, tweet_features, lens = inputs\n",
    "\n",
    "        labels = labels#.to(device)\n",
    "        tweet_features = tweet_features#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(tweet_features, lens, labels)\n",
    "\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        loss = loss_function(output, labels, loss_type, expt_type, scale)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        fin_conf.append(output.cpu().detach().numpy())\n",
    "\n",
    "        fin_targets.append(labels.cpu().detach().numpy())\n",
    "        fin_outputs.append(preds.cpu().detach().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_corrects.double() / dataset_len\n",
    "\n",
    "    return epoch_loss, epoch_accuracy, np.hstack(fin_outputs), np.hstack(fin_targets), fin_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7475664",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = time.time()\n",
    "model.load_state_dict(best_model_wts)\n",
    "_, _, y_pred, y_true, conf = eval_loop(model,\n",
    "                                       expt_type,\n",
    "                                       test_dataloader,\n",
    "                                       device,\n",
    "                                       len(test_dataset),\n",
    "                                       loss_type,\n",
    "                                       scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4786188f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7142857142857143, 0.21739130434782608, 0.3333333333333333, 0.51)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = gr_metrics(y_pred, y_true)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9436843f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7777777725925925, 0.46153845763313617, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classwise_FScores = class_FScore(y_pred, y_true, expt_type)\n",
    "classwise_FScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "613900d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Precision' in metrics_dict:\n",
    "    metrics_dict['Precision'].append(m[0])\n",
    "    metrics_dict['Recall'].append(m[1])\n",
    "    metrics_dict['FScore'].append(m[2])\n",
    "else:\n",
    "    metrics_dict['Precision'] = [m[0]]\n",
    "    metrics_dict['Recall'] = [m[1]]\n",
    "    metrics_dict['FScore'] = [m[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "debbeafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>FScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall    FScore\n",
       "0   0.714286  0.217391  0.333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(metrics_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3547e",
   "metadata": {},
   "source": [
    "# 4. 뒤부터 풀어보기, model부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecdc6606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedditModel(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc_2): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  (historic_model): MyLSTM(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm1): LSTM(768, 512, bidirectional=True)\n",
       "    (atten1): Attention()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01674934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8026779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditModel(nn.Module):\n",
    "    def __init__(self, op_units=5, embedding_dim=768, hidden_dim=128, lstm_layer=1, dropout=0.5):\n",
    "        super(RedditModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "        self.fc_2 = nn.Linear(hidden_dim*2, op_units)\n",
    "\n",
    "        self.historic_model = MyLSTM(self.embedding_dim, self.hidden_dim, lstm_layer, dropout)\n",
    "\n",
    "    def get_pred(self, feat):\n",
    "        feat = self.fc_1(self.dropout(feat))\n",
    "        return self.fc_2(feat)\n",
    "\n",
    "    def forward(self, tweets, lengths, labels):\n",
    "        h, _ = self.historic_model(tweets, lengths)\n",
    "        if h.dim() == 1:\n",
    "            h = h.unsqueeze(0)\n",
    "        e = self.get_pred(h)\n",
    "\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd1a3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_units=5\n",
    "embedding_dim=768\n",
    "hidden_dim=128\n",
    "lstm_layer=1\n",
    "dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1a54f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeced5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=256, bias=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_1 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "fc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9e6de57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=5, bias=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_2 = nn.Linear(hidden_dim*2, op_units)\n",
    "fc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e12e53",
   "metadata": {},
   "source": [
    "## 4.1 MyLSTM 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb530d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_dim=128, lstm_layer=1, dropout=0.6):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm1 = nn.LSTM(input_size=self.embedding_dim,\n",
    "                             hidden_size=hidden_dim,\n",
    "                             num_layers=lstm_layer,\n",
    "                             bidirectional=True)\n",
    "        self.atten1 = Attention(hidden_dim*2, batch_first=True)  # 2 is bidrectional\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "        out, (h_n, c_n) = self.lstm1(x)\n",
    "        x, lengths = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        x, _ = self.atten1(x, lengths)  # skip connect\n",
    "\n",
    "        return x, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9defdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=768\n",
    "hidden_dim=128\n",
    "lstm_layer=1\n",
    "dropout=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0868dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bea6c9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(768, 128, bidirectional=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LSTM(input_size=embedding_dim,\n",
    "                 hidden_size=hidden_dim,\n",
    "                 num_layers=lstm_layer,\n",
    "                 bidirectional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833aeaa",
   "metadata": {},
   "source": [
    "### 4.1.1 Attention 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94a9a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_first=False):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.att_weights:\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def get_mask(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        if self.batch_first:\n",
    "            batch_size, max_len = inputs.size()[:2]\n",
    "        else:\n",
    "            max_len, batch_size = inputs.size()[:2]\n",
    "\n",
    "        # apply attention layer\n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.att_weights  # (1, hidden_size)\n",
    "                            .permute(1, 0)  # (hidden_size, 1)\n",
    "                            .unsqueeze(0)  # (1, hidden_size, 1)\n",
    "                            .repeat(batch_size, 1, 1)  # (batch_size, hidden_size, 1)\n",
    "                            )\n",
    "\n",
    "        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n",
    "\n",
    "        # create mask based on the sentence lengths\n",
    "#         mask = torch.ones(attentions.size(), requires_grad=False).to('cpu')\n",
    "#         for i, l in enumerate(lengths):  # skip the first sentence\n",
    "#             if l < max_len:\n",
    "#                 mask[i, l:] = 0\n",
    "\n",
    "#         # apply mask and renormalize attention scores (weights)\n",
    "#         masked = attentions * mask\n",
    "#         _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n",
    "\n",
    "#         attentions = masked.div(_sums)\n",
    "\n",
    "        # if attentions.dim() == 1:\n",
    "        #     attentions = attentions.unsqueeze(1)\n",
    "\n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
    "\n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "\n",
    "        return representations, attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d517a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = hidden_dim*2\n",
    "batch_first = True\n",
    "\n",
    "att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "\n",
    "stdv = 1.0 / np.sqrt(hidden_size)\n",
    "for weight in att_weights:\n",
    "    nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "def get_mask(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5385ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bi, inputs in enumerate(train_dataloader):\n",
    "        labels, tweet_features, lens = inputs\n",
    "\n",
    "        labels = labels#.to(device)\n",
    "        tweet_features = tweet_features#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da74e87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebb4cfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cce0dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 54, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ebd78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[8],[8, 28, 768],[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4ac9165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8], [8, 28, 768]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49a1c73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2, 1, 2, 1, 2, 1, 2, 1]),\n",
       " tensor([[[-0.0553,  0.1175,  0.0335,  ..., -0.0765,  0.0150, -0.0386],\n",
       "          [-0.0865,  0.0567, -0.0039,  ..., -0.0753, -0.0367, -0.0458],\n",
       "          [-0.0556,  0.0892, -0.0098,  ..., -0.1102,  0.0048, -0.0400],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.1390,  0.0848,  0.0417,  ..., -0.0604, -0.0009, -0.0137],\n",
       "          [-0.1313,  0.0552,  0.0709,  ..., -0.1226,  0.0031,  0.0345],\n",
       "          [-0.0814,  0.0769,  0.0271,  ..., -0.0803, -0.0298, -0.0536],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.1092,  0.1412,  0.0492,  ..., -0.0746, -0.0007,  0.0140],\n",
       "          [-0.0918,  0.0730,  0.0499,  ..., -0.0891, -0.0033, -0.0402],\n",
       "          [-0.0572,  0.1136,  0.0444,  ..., -0.1158, -0.0455, -0.0178],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0625,  0.0989, -0.0008,  ..., -0.1157, -0.0335, -0.0167],\n",
       "          [-0.1706,  0.0602,  0.0022,  ..., -0.1169,  0.0075, -0.0142],\n",
       "          [-0.1269,  0.0671, -0.0094,  ..., -0.1607, -0.0144, -0.0110],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0730,  0.0826,  0.0362,  ..., -0.0623, -0.0003, -0.0016],\n",
       "          [-0.0519,  0.0783,  0.0066,  ..., -0.0590, -0.0153, -0.0010],\n",
       "          [-0.1119,  0.0424,  0.0253,  ..., -0.0619,  0.0028, -0.0128],\n",
       "          ...,\n",
       "          [-0.1378,  0.0733,  0.0349,  ..., -0.0956,  0.0050, -0.0234],\n",
       "          [-0.0565,  0.1060, -0.0040,  ..., -0.1116, -0.0163, -0.0225],\n",
       "          [-0.0767,  0.0498,  0.0385,  ..., -0.0752,  0.0196,  0.0138]],\n",
       " \n",
       "         [[-0.0780,  0.1083,  0.0022,  ..., -0.1505, -0.0203, -0.0765],\n",
       "          [-0.1254,  0.0622,  0.0520,  ..., -0.0569, -0.0012,  0.0141],\n",
       "          [-0.0615,  0.0606,  0.0355,  ..., -0.0916, -0.0007, -0.0006],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]),\n",
       " tensor([ 7, 14,  8,  1,  3,  4, 54, 26])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e795c4ac",
   "metadata": {},
   "source": [
    "batch_size, max_len = inputs.size()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4fddcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten1 = Attention(hidden_dim*2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7266b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_model = MyLSTM(embedding_dim, hidden_dim, lstm_layer, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c574605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       "  (lstm1): LSTM(768, 128, bidirectional=True)\n",
       "  (atten1): Attention()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acaff65",
   "metadata": {},
   "source": [
    "# 코드 푸는 순서가 Redditmodel <- MyLSTM <- Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97679f4",
   "metadata": {},
   "source": [
    "# 다시 처음부터\n",
    "# 5. data 구조 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9620f4",
   "metadata": {},
   "source": [
    "### dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9a202",
   "metadata": {},
   "source": [
    "### 데이터배치와 반복자 생성하기\n",
    "https://tutorials.pytorch.kr/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "1. collate_fn()\n",
    "    * 데이터 로더에서 미니배치 생성시, 배치 샘플러가 뽑은 데이터셋들을 합치는 역할\n",
    "    * 모델로 보내기 전, DataLoader에서 생성된 샘플 배치로 동작함\n",
    "    * collate_fn의 입력은 DataLoader에 배치 크기가 있는 배치 데이터임\n",
    "    * 미리 선언된 데이터 처리 파이프라인에 따라 처리\n",
    "\n",
    "2. def pad_collate_reddit(batch): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b623cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def pad_collate_reddit(batch): \n",
    "    target = [item[0] for item in batch]\n",
    "    tweet = [item[1] for item in batch]\n",
    "\n",
    "    lens = [len(x) for x in tweet]\n",
    "\n",
    "    tweet = nn.utils.rnn.pad_sequence(tweet, batch_first=True, padding_value=0)\n",
    "\n",
    "    target = torch.tensor(target)\n",
    "    lens = torch.tensor(lens)\n",
    "\n",
    "    return [target, tweet, lens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec6a07",
   "metadata": {},
   "source": [
    "### custom dataset(커스텀 데이터셋)\n",
    "\n",
    "https://wikidocs.net/57165\n",
    "\n",
    "* 데이터셋을 상속받아 직접 커스텀 데이터셋을 만드는 것으로\n",
    "* torch.utils.data.Dataset은 파이토치에서 데이터셋을 제공하는 추상 클래스\n",
    "\n",
    "#### 커스텀 데이터셋 기본 뼈대\n",
    "* class CustomDataset(torch.utils.data.Dataset): \n",
    "  * def __init__(self):           # 데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  * def __len__(self):            # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "                                # 데이터셋의 크기를 리턴할 len\n",
    "  * def __getitem__(self, idx):  # 데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "                               # dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 get_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c366c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, labels, tweets, days=200):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.tweets = tweets # 트위터의 게시물\n",
    "        self.days = days\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        labels = torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        if self.days > len(self.tweets[item]):\n",
    "            tweets = torch.tensor(self.tweets[item], dtype=torch.float32)\n",
    "        else:\n",
    "            tweets = torch.tensor(self.tweets[item][:self.days], dtype=torch.float32)\n",
    "        \n",
    "        return [labels, tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e95bb",
   "metadata": {},
   "source": [
    "## RedditDataset 코드 풀어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f22db",
   "metadata": {},
   "source": [
    "### def __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ffc2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df_train.label.values \n",
    "tweets=df_train.enc.values\n",
    "days=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4ac00",
   "metadata": {},
   "source": [
    "### def __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "914ef261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "item=random.randint(1,400) # 1-400 사이의 랜덤 정수 가져오기\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b10e91d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor(labels[item], dtype=torch.long)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f72e446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if days > len(tweets[item]):\n",
    "    tweets = torch.tensor(tweets[item], dtype=torch.float32)\n",
    "else:\n",
    "    tweets = torch.tensor(tweets[item][:days], dtype=torch.float32)\n",
    "\n",
    "[labels, tweets]\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c704ef",
   "metadata": {},
   "source": [
    "#### if문을 해주는 이유\n",
    "* 해당 데이터의 경우 500명의 reddit posts이다.\n",
    "\n",
    "* enc는 각 게시물에 대해서 768차원으로 인코딩한 리스트 -> enc를 tweet이라고 정의\n",
    "* 게시물의 길이가 200보다 긴 경우, 200까지만 나오도록 조정해서 반환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78265bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(df_train.Post)) # 사용자 중 가장 많은 게시물을 쓴 사람은 24개."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4a728e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.Post[3])) # 4 | 3번 사용자의 경우, 4개의 게시물이 있다\n",
    "len(df_train.enc[3][0]) # 768\n",
    "len(df_train.enc[3][1]) # 768\n",
    "len(df_train.enc[3][2]) # 768\n",
    "len(df_train.enc[3][3]) # 768\n",
    "# len(df.enc[3][4]) # 3번 사용자는 게시물이 4개까지 있기에 0,1,2,3 -> 4는 존재하지 않는다\n",
    "len(df_train.enc[3]) # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "093d8c41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets[3]) # 768, else문 진행\n",
    "# tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "daf4c0aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2773432/3627452317.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  len(torch.tensor(tweets[3][:days], dtype=torch.float32)) # 200\n",
      "/tmp/ipykernel_2773432/3627452317.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(tweets[3][:days], dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-4.3798e-02,  3.6652e-02, -1.2237e-02, -8.9592e-02,  1.1714e-01,\n",
       "        -5.9329e-02, -2.4218e-02,  1.3143e-02,  4.5902e-02, -9.9838e-02,\n",
       "        -4.4578e-02,  5.1750e-02,  1.9876e-02, -2.5739e-02,  4.7524e-02,\n",
       "        -2.2292e-02, -6.2777e-02,  2.0296e-02,  1.6537e-02, -4.5113e-02,\n",
       "        -6.7061e-02,  4.2000e-02,  1.4737e-03,  1.1524e-01,  1.1123e-02,\n",
       "         3.3623e-02,  1.0772e-01,  1.4821e-01, -5.2686e-02, -4.2978e-03,\n",
       "        -5.9954e-02, -6.1087e-02,  9.4013e-02, -9.2577e-03, -9.9320e-03,\n",
       "         7.5362e-02,  8.9905e-02, -3.8388e-02, -8.4326e-02, -4.7914e-03,\n",
       "         3.3067e-02,  2.7983e-02,  2.2044e-02, -3.6369e-02,  5.1264e-02,\n",
       "        -1.0881e-02, -1.3514e-02, -3.3244e-04, -1.1853e-02,  4.1809e-03,\n",
       "         3.7452e-02,  3.2590e-02, -4.5625e-02,  4.6694e-02, -9.5488e-02,\n",
       "         8.0763e-02,  2.6517e-02,  3.1645e-02,  6.2567e-02, -4.7160e-02,\n",
       "        -2.7078e-02, -8.1448e-02, -8.3191e-02, -4.0581e-02,  6.5065e-02,\n",
       "        -1.4841e-02, -3.2725e-02, -4.6252e-02, -9.9694e-03,  6.9689e-02,\n",
       "         1.6294e-02, -2.1722e-02,  9.2076e-04, -3.2565e-02, -1.4817e-02,\n",
       "        -9.6323e-03,  2.2936e-02,  2.8520e-01, -1.6414e-01, -1.9054e-02,\n",
       "         4.7985e-02, -1.0334e-01,  2.6527e-01,  4.6518e-02, -1.4534e-02,\n",
       "        -3.0728e-02,  7.3710e-02, -7.0768e-03,  5.9627e-03,  3.3584e-02,\n",
       "         3.3303e-02,  5.1704e-02, -2.4239e-02, -3.3131e-02,  2.8745e-02,\n",
       "         3.8490e-02,  1.9226e-02, -1.9482e-01, -2.4800e-02, -6.7108e-02,\n",
       "        -1.4811e-02, -7.8269e-02,  5.9649e-02,  6.3074e-02, -1.2351e-02,\n",
       "         1.5172e-02,  4.4797e-02, -5.9329e-02,  6.1421e-02, -4.6293e-03,\n",
       "        -7.9954e-04,  1.3612e-02,  6.5452e-02, -1.7944e-02, -2.8634e-02,\n",
       "        -2.9339e-02, -1.7020e-02,  2.7913e-02,  2.5904e-02,  6.3367e-02,\n",
       "         4.5382e-03,  7.6295e-02,  1.2931e-01, -3.6108e-02, -5.1750e-02,\n",
       "        -2.3309e-02, -5.0281e-02,  8.5887e-03,  8.9822e-03,  2.9657e-02,\n",
       "        -1.8790e-02, -2.1091e-01, -5.4751e-02,  2.7751e-02,  2.7891e-02,\n",
       "         3.7058e-02,  8.0857e-02, -5.7111e-02,  6.7852e-02, -1.6771e-02,\n",
       "         2.4003e-03, -1.7257e-02,  3.3077e-02,  2.3318e-02,  7.0864e-02,\n",
       "         2.7094e-02,  1.2054e-02, -7.4858e-02,  4.9822e-02,  9.1214e-03,\n",
       "         3.5911e-02, -1.2740e-01, -8.2636e-03,  4.1971e-02, -8.3048e-02,\n",
       "         4.8894e-01,  1.0026e-01, -1.7168e-02, -2.2300e-02,  2.5390e-02,\n",
       "         1.3227e-01,  9.5370e-03,  1.4990e-02, -1.9472e-02, -3.0936e-02,\n",
       "        -1.7399e-02, -3.6216e-02, -2.3032e-02,  4.2674e-02,  3.9019e-02,\n",
       "         9.6903e-02,  2.3452e-02,  8.2065e-02,  1.1692e-02, -8.4144e-02,\n",
       "        -4.9543e-02,  3.2048e-02,  1.0729e-02, -7.0016e-02,  1.3761e-02,\n",
       "         1.2827e-02,  6.2045e-02, -4.8786e-02,  6.1402e-02, -4.7336e-02,\n",
       "         2.5689e-02,  3.9790e-02,  3.4647e-02,  1.6254e-02,  4.3748e-02,\n",
       "         5.4295e-02,  3.4515e-02,  7.5051e-03, -2.5876e-02, -6.4348e-03,\n",
       "         8.1624e-02,  2.1461e-03, -5.2673e-02,  4.6717e-02, -8.5047e-02])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.tensor(tweets[3][:days], dtype=torch.float32)) # 200\n",
    "torch.tensor(tweets[3][:days], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e95111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0438)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed4a4849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RedditDataset at 0x7f02baad2d90>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "train_dataset = RedditDataset(\n",
    "            df_train.label.values, # labels에 해당\n",
    "            df_train.enc.values    # posts에 해당\n",
    "            ) \n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1da2d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, collate_fn=pad_collate_reddit, shuffle=True) \n",
    "\n",
    "test_dataset = RedditDataset(\n",
    "            df_test.label.values, df_test.enc.values)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, collate_fn=pad_collate_reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fbb2d",
   "metadata": {},
   "source": [
    "# 6. model 풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3692edc",
   "metadata": {},
   "source": [
    "## 6.1 Input 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcce05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_train_data=next(iter(train_dataloader))\n",
    "inputs=second_train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90b86914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.6060e-02,  6.0873e-02,  3.8409e-02,  ..., -1.1287e-01,\n",
       "          -2.7783e-02,  2.3558e-02],\n",
       "         [-7.9226e-02,  7.7066e-02,  9.7964e-03,  ..., -1.1475e-01,\n",
       "          -3.7525e-02, -1.2379e-03],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-1.1699e-01,  4.7306e-02,  6.1936e-02,  ..., -6.7025e-02,\n",
       "          -2.2637e-03,  3.5783e-02],\n",
       "         [-7.1922e-02,  8.4512e-02,  3.7833e-02,  ..., -1.5439e-02,\n",
       "          -4.2933e-02,  9.0660e-02],\n",
       "         [-9.8161e-02,  9.3266e-02, -1.3176e-02,  ..., -6.3279e-02,\n",
       "          -5.7189e-02, -9.3285e-02],\n",
       "         ...,\n",
       "         [-1.0713e-01,  4.0313e-02,  2.1937e-02,  ..., -1.4032e-01,\n",
       "          -1.4235e-02, -7.0426e-03],\n",
       "         [-6.4083e-02,  2.5268e-02,  1.5963e-02,  ..., -4.5176e-02,\n",
       "          -1.0134e-02,  3.0034e-02],\n",
       "         [-1.0751e-01,  6.5598e-02,  3.3021e-03,  ..., -1.6617e-01,\n",
       "          -4.9436e-02, -3.4368e-02]],\n",
       "\n",
       "        [[-9.7217e-02,  8.8377e-02, -7.0270e-03,  ..., -1.4081e-01,\n",
       "          -6.1185e-02, -4.4872e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.9277e-02,  6.6777e-02,  4.4742e-03,  ..., -6.2229e-02,\n",
       "          -5.4891e-02, -2.9902e-02],\n",
       "         [-9.9943e-02,  9.1704e-02, -1.4790e-04,  ..., -1.3296e-01,\n",
       "          -8.7377e-02, -1.7795e-02],\n",
       "         [-6.3932e-02,  8.1757e-02, -1.9275e-02,  ..., -1.1337e-01,\n",
       "          -4.3759e-02, -5.8047e-02],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-6.9855e-02,  9.0318e-02, -1.0810e-03,  ..., -5.6312e-02,\n",
       "           6.2312e-03, -5.5711e-02],\n",
       "         [-8.0051e-02,  1.2013e-01,  2.9970e-02,  ..., -4.7970e-02,\n",
       "           1.1579e-02, -7.7905e-03],\n",
       "         [-1.2408e-01,  1.1192e-01,  1.4121e-02,  ..., -6.0418e-02,\n",
       "          -4.8175e-02, -5.0848e-02],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-3.4763e-02,  1.1913e-01,  1.2075e-02,  ..., -7.5590e-02,\n",
       "           5.8507e-03, -1.2254e-02],\n",
       "         [-6.6566e-02,  1.0818e-01, -4.2810e-03,  ..., -3.7937e-02,\n",
       "           1.5409e-02, -2.5471e-02],\n",
       "         [-4.4480e-02,  1.0690e-01, -1.0098e-02,  ..., -6.3347e-02,\n",
       "          -4.9549e-03, -8.9167e-02],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "163bc198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46f2a2",
   "metadata": {},
   "source": [
    "## 6.2 class Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbbd42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_first=False):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.att_weights:\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def get_mask(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        if self.batch_first:\n",
    "            batch_size, max_len = inputs.size()[:2]\n",
    "        else:\n",
    "            max_len, batch_size = inputs.size()[:2]\n",
    "\n",
    "        # apply attention layer\n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.att_weights  # (1, hidden_size)\n",
    "                            .permute(1, 0)  # (hidden_size, 1)\n",
    "                            .unsqueeze(0)  # (1, hidden_size, 1)\n",
    "                            .repeat(batch_size, 1, 1)  # (batch_size, hidden_size, 1)\n",
    "                            )\n",
    "\n",
    "        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n",
    "\n",
    "        # create mask based on the sentence lengths\n",
    "        mask = torch.ones(attentions.size(), requires_grad=True).to(device)# .cuda()\n",
    "        for i, l in enumerate(lengths):  # skip the first sentence\n",
    "            if l < max_len:\n",
    "                mask[i, l:] = 0\n",
    "\n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        masked = attentions * mask\n",
    "        _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n",
    "\n",
    "        attentions = masked.div(_sums)\n",
    "\n",
    "        # if attentions.dim() == 1:\n",
    "        #     attentions = attentions.unsqueeze(1)\n",
    "\n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
    "\n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "\n",
    "        return representations, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb63b9b",
   "metadata": {},
   "source": [
    "## Class Attention 코드 풀어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da32126e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hidden_size = 768 # hidden_size를 정의해준 부분이 없음 ??\n",
    "batch_first=False # class마다 True, False가 다르므로 각각 명시해주어야 함\n",
    "att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "stdv = 1.0 / np.sqrt(hidden_size)\n",
    "\n",
    "for weight in att_weights:\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "att_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f12fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e9dea7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_train_data=next(iter(train_dataloader))\n",
    "inputs=second_train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a00dd6a-ff51-4434-85b3-48d53c3905bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = inputs.size()[:2] # [:2] 가 무슨기능????????//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fedb69e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03e0893c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c626a00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49a2eff5-c60f-432c-9661-5949cde3d19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.bmm(inputs,\n",
    "                    att_weights  # (1, hidden_size)\n",
    "                    .permute(1, 0)  # (hidden_size, 1)\n",
    "                    .unsqueeze(0)  # (1, hidden_size, 1)\n",
    "                    .repeat(batch_size, 1, 1)  # (batch_size, hidden_size, 1)\n",
    "                    )\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dab4a090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.squeeze().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6aaee70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n",
    "attentions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75e273e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
    "weighted.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e1f9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representations = weighted.sum(1).squeeze()\n",
    "representations.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0178a",
   "metadata": {},
   "source": [
    "## 6.3 MyLSTM 코드 풀어보기\n",
    "* Redditmodel에서 MyLSTM(self.embedding_dim, self.hidden_dim, lstm_layer, dropout)으로 쓰임\n",
    "* historic_model(tweets, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a89afef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_dim=128, lstm_layer=1, dropout=0.6):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm1 = nn.LSTM(input_size=self.embedding_dim,\n",
    "                             hidden_size=hidden_dim,\n",
    "                             num_layers=lstm_layer,\n",
    "                             bidirectional=True)\n",
    "        self.atten1 = Attention(hidden_dim*2, batch_first=True)  # 2 is bidrectional\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "        out, (h_n, c_n) = self.lstm1(x)\n",
    "        x, lengths = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        x, _ = self.atten1(x, lengths)  # skip connect\n",
    "\n",
    "        return x, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4189d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=768\n",
    "hidden_dim=128\n",
    "lstm_layer=1\n",
    "dropout=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6f99eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f09f15a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(768, 128, bidirectional=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm1 = nn.LSTM(input_size=embedding_dim,\n",
    "                 hidden_size=hidden_dim,\n",
    "                 num_layers=lstm_layer,\n",
    "                 bidirectional=True)\n",
    "lstm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8350b858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten1 = Attention(hidden_dim*2, batch_first=True)\n",
    "atten1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6e238",
   "metadata": {},
   "source": [
    "forward의 x는 tweets임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5286e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=second_train_data[1]\n",
    "x_len=second_train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a78036a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 768])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=dropout(x)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bebaf1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.1392,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.1102],\n",
       "        [-0.0000,  0.0000, -0.0146,  ..., -0.3217, -0.0000, -0.0000],\n",
       "        [-0.1089,  0.2633,  0.0000,  ..., -0.0000,  0.0188, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.2827,  0.0000,  ..., -0.0000, -0.0000, -0.0844],\n",
       "        [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [-0.0640,  0.0000,  0.0599,  ..., -0.0000, -0.0000, -0.0000]]), batch_sizes=tensor([8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 5, 5, 5, 3, 3,\n",
       "        3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([7, 5, 0, 4, 6, 3, 1, 2]), unsorted_indices=tensor([2, 6, 7, 5, 3, 1, 4, 0]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7af07e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (h_n, c_n) = lstm1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cdc876a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.0238,  0.0403, -0.0090,  ..., -0.0942,  0.1965, -0.0975],\n",
       "        [-0.0327, -0.0106,  0.0120,  ..., -0.0731,  0.2535, -0.1225],\n",
       "        [-0.0123,  0.0099,  0.0963,  ..., -0.1105,  0.2100, -0.3535],\n",
       "        ...,\n",
       "        [-0.0514,  0.0154,  0.1713,  ..., -0.1770,  0.1341, -0.3069],\n",
       "        [-0.0328,  0.0184,  0.1802,  ..., -0.1106,  0.0651, -0.2461],\n",
       "        [-0.0163,  0.0553,  0.2955,  ..., -0.0127, -0.0314, -0.0466]],\n",
       "       grad_fn=<CatBackward0>), batch_sizes=tensor([8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 5, 5, 5, 3, 3,\n",
       "        3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([7, 5, 0, 4, 6, 3, 1, 2]), unsorted_indices=tensor([2, 6, 7, 5, 3, 1, 4, 0]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56ea0b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0459,  0.0664,  0.0158,  ...,  0.0294,  0.1332,  0.0370],\n",
       "          [-0.0163,  0.0108,  0.1185,  ..., -0.0130,  0.2359,  0.0730],\n",
       "          [-0.0201,  0.0121,  0.1176,  ..., -0.0149,  0.1274, -0.0395],\n",
       "          ...,\n",
       "          [-0.0083,  0.0177,  0.0898,  ..., -0.0201,  0.4023,  0.0788],\n",
       "          [ 0.0283,  0.0650,  0.2173,  ..., -0.0226,  0.2053,  0.1024],\n",
       "          [-0.0163,  0.0553,  0.2955,  ..., -0.1477,  0.2371,  0.1710]],\n",
       " \n",
       "         [[-0.1586,  0.3025, -0.0340,  ..., -0.1105,  0.2100, -0.3535],\n",
       "          [ 0.1394, -0.0116,  0.0401,  ..., -0.0131,  0.0197, -0.1109],\n",
       "          [-0.0487,  0.1723,  0.0039,  ..., -0.0881,  0.0933, -0.2178],\n",
       "          ...,\n",
       "          [ 0.0953,  0.3639,  0.0100,  ..., -0.0731,  0.2535, -0.1225],\n",
       "          [ 0.0357,  0.2682,  0.0586,  ..., -0.0024,  0.1811, -0.1722],\n",
       "          [ 0.1699,  0.2039,  0.0097,  ..., -0.0942,  0.1965, -0.0975]]],\n",
       "        grad_fn=<IndexSelectBackward0>),\n",
       " tensor([[[ 0.0925,  0.1232,  0.0307,  ...,  0.0625,  0.2995,  0.0742],\n",
       "          [-0.1520,  0.0687,  0.4982,  ..., -0.1340,  0.2611,  0.1055],\n",
       "          [-0.1561,  0.0635,  0.5925,  ..., -0.1989,  0.1380, -0.0569],\n",
       "          ...,\n",
       "          [-0.0680,  0.1043,  0.3972,  ..., -0.2621,  0.4687,  0.1048],\n",
       "          [ 0.0505,  0.1213,  0.5624,  ..., -0.0700,  0.4689,  0.2065],\n",
       "          [-0.0299,  0.1149,  0.7946,  ..., -0.3918,  0.6758,  0.3009]],\n",
       " \n",
       "         [[-0.2070,  1.3290, -0.0399,  ..., -0.3482,  0.7239, -0.5153],\n",
       "          [ 0.2513, -0.0230,  0.0729,  ..., -0.0254,  0.0426, -0.2274],\n",
       "          [-0.0665,  0.6374,  0.0048,  ..., -0.3250,  0.2585, -0.2970],\n",
       "          ...,\n",
       "          [ 0.2004,  0.8708,  0.0199,  ..., -0.1201,  0.7336, -0.3064],\n",
       "          [ 0.0691,  0.6136,  0.1124,  ..., -0.0038,  0.3965, -0.4573],\n",
       "          [ 0.3363,  0.4310,  0.0189,  ..., -0.1704,  0.4276, -0.2121]]],\n",
       "        grad_fn=<IndexSelectBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e960df2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 130, 256])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925443a",
   "metadata": {},
   "source": [
    "## 6.4 RedditModel 코드 풀어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e1dba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model_type'] == 'lstm+att'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a57ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditModel(nn.Module):\n",
    "    def __init__(self, op_units=5, embedding_dim=768, hidden_dim=128, lstm_layer=1, dropout=0.5):\n",
    "        super(RedditModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "        self.fc_2 = nn.Linear(hidden_dim*2, op_units)\n",
    "\n",
    "        self.historic_model = MyLSTM(self.embedding_dim, self.hidden_dim, lstm_layer, dropout)\n",
    "\n",
    "    def get_pred(self, feat):\n",
    "        feat = self.fc_1(self.dropout(feat))\n",
    "        return self.fc_2(feat)\n",
    "\n",
    "    def forward(self, tweets, lengths, labels):\n",
    "        h, _ = self.historic_model(tweets, lengths)\n",
    "        if h.dim() == 1:\n",
    "            h = h.unsqueeze(0)\n",
    "        e = self.get_pred(h)\n",
    "\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "075f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_units=5\n",
    "embedding_dim=768\n",
    "hidden_dim=128\n",
    "lstm_layer=1\n",
    "dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0254201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=0.5)\n",
    "fc_1 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "fc_2 = nn.Linear(hidden_dim*2, op_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d241a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_model = MyLSTM(embedding_dim, hidden_dim, lstm_layer, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a778424",
   "metadata": {},
   "source": [
    "forward 진행 전 인풋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2aa99712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([4, 1, 2, 2, 3, 2, 1, 1]),\n",
       " tensor([[[-0.0436,  0.1053,  0.0006,  ..., -0.0890,  0.0075, -0.1056],\n",
       "          [-0.0679,  0.0835,  0.0467,  ..., -0.0658, -0.0073,  0.0197],\n",
       "          [-0.0689,  0.0453,  0.0218,  ..., -0.0969, -0.0332, -0.0208],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0753,  0.0501, -0.0393,  ..., -0.1116, -0.0169,  0.0050],\n",
       "          [-0.0898,  0.0147, -0.0285,  ..., -0.1240, -0.0172,  0.0275],\n",
       "          [-0.0141,  0.0909,  0.0078,  ..., -0.1082,  0.0224, -0.0661],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.1121,  0.1177,  0.0401,  ..., -0.0817,  0.0015,  0.0080],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0804,  0.0849, -0.0058,  ..., -0.1287, -0.0014, -0.0248],\n",
       "          [-0.0960,  0.0931,  0.0117,  ..., -0.0983, -0.0008, -0.0293],\n",
       "          [-0.0956,  0.0875,  0.0295,  ..., -0.0664, -0.0101, -0.0414],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.1170,  0.0473,  0.0619,  ..., -0.0670, -0.0023,  0.0358],\n",
       "          [-0.0719,  0.0845,  0.0378,  ..., -0.0154, -0.0429,  0.0907],\n",
       "          [-0.0982,  0.0933, -0.0132,  ..., -0.0633, -0.0572, -0.0933],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0557,  0.0820,  0.0334,  ..., -0.0605, -0.0181, -0.0441],\n",
       "          [-0.0691,  0.1007,  0.0127,  ..., -0.0723, -0.0159, -0.0277],\n",
       "          [-0.0748,  0.1208,  0.0223,  ..., -0.0467, -0.0293, -0.0607],\n",
       "          ...,\n",
       "          [-0.0771,  0.1131,  0.0215,  ..., -0.0559, -0.0116, -0.0338],\n",
       "          [-0.0677,  0.0716,  0.0244,  ..., -0.0944, -0.0584, -0.0146],\n",
       "          [-0.0256,  0.0727,  0.0240,  ..., -0.0895, -0.0119, -0.0240]]]),\n",
       " tensor([ 29,  17,   1,  19,  22,  42,  22, 130])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9a71421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables=second_train_data[0]\n",
    "tweets=second_train_data[1]\n",
    "lengths=second_train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13f3c49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h, _ \u001b[38;5;241m=\u001b[39m \u001b[43mhistoric_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hyolim/hyolim_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36mMyLSTM.forward\u001b[0;34m(self, x, x_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm1(x)\n\u001b[1;32m     17\u001b[0m x, lengths \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(out, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matten1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# skip connect\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, _\n",
      "File \u001b[0;32m~/hyolim/hyolim_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, inputs, lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lengths):  \u001b[38;5;66;03m# skip the first sentence\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m<\u001b[39m max_len:\n\u001b[0;32m---> 37\u001b[0m         mask[i, l:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# apply mask and renormalize attention scores (weights)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m masked \u001b[38;5;241m=\u001b[39m attentions \u001b[38;5;241m*\u001b[39m mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "h, _ = historic_model(tweets, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df02a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(self, feat):\n",
    "    feat = self.fc_1(self.dropout(feat))\n",
    "    return self.fc_2(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "e = get_pred(h)\n",
    "e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyolim_py38",
   "language": "python",
   "name": "hyolim_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
