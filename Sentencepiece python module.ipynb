{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6b90a3",
   "metadata": {},
   "source": [
    "# SentencePiece Python module\n",
    "* SentencePiece: 내부 단어 분리를 위한 유용한 패키지\n",
    "*참고: https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=T9BDzLVkUFT4\n",
    "https://wikidocs.net/86657"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a3274",
   "metadata": {},
   "source": [
    "## SentencePiece 학습 코드\n",
    "* 참고: http://tensorboy.com/bpe-sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00288ffe",
   "metadata": {},
   "source": [
    "* 입력: corpus(말뭉치), prefix(생성할 파일 이름), vocab_size(vocabulary 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6fad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentencepiece(corpus, prefix, vocab_size=32000):\n",
    "    \"\"\"\n",
    "    sentencepiece를 이용해 vocab 학습\n",
    "    :param corpus: 학습할 말뭉치\n",
    "    :param prefix: 저장할 vocab 이름\n",
    "    :param vocab_size: vocab 개수\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" +  # 7은 특수문자 개수\n",
    "        \" --model_type=unigram\" +\n",
    "        \" --max_sentence_length=999999\" +  # 문장 최대 길이\n",
    "        \" --pad_id=0 --pad_piece=[PAD]\" +  # pad token 및 id 지정\n",
    "        \" --unk_id=1 --unk_piece=[UNK]\" +  # unknown token 및 id 지정\n",
    "        \" --bos_id=2 --bos_piece=[BOS]\" +  # begin of sequence token 및 id 지정\n",
    "        \" --eos_id=3 --eos_piece=[EOS]\" +  # end of sequence token 및 id 지정\n",
    "        \" --user_defined_symbols=[SEP],[CLS],[MASK]\" +  # 기타 추가 토큰 SEP: 4, CLS: 5, MASK: 6\n",
    "        \" --input_sentence_size=100000\" +  # 말뭉치에서 셈플링해서 학습\n",
    "        \" --shuffle_input_sentence=true\")  # 셈플링한 말뭉치 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa883cc0",
   "metadata": {},
   "source": [
    "함수 시행 시, 다음 두 파일 생성\n",
    "* '<'prefix'>.model': Sentencepiece가 학습한 내용을 읽어 들이기 위한 파일.\n",
    "* '<'prefix'>.vocab': text 형식으로 내용을 확인할 수 있는 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94965e0",
   "metadata": {},
   "source": [
    "## Install and data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e11a3a",
   "metadata": {},
   "source": [
    "training data: botchan.txt, English-translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a53ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-08 10:30:01--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278779 (272K) [text/plain]\n",
      "Saving to: ‘botchan.txt’\n",
      "\n",
      "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-09-08 10:30:01 (5.55 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2bf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63d3a8",
   "metadata": {},
   "source": [
    "## Basic end-to-end example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ff4fb",
   "metadata": {},
   "source": [
    "### train \n",
    "Vocab 만들기 |\n",
    "센텐스피스로 단어 집합과 각 단어에 고유한 정수를 부여하기\n",
    "* input : 학습시킬 파일\n",
    "* model_prefix : 만들어질 모델 이름\n",
    "* vocab_size : 단어 집합의 크기\n",
    "------\n",
    "* model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
    "* max_sentence_length: 문장의 최대 길이\n",
    "* pad_id, pad_piece: pad token id, 값\n",
    "* unk_id, unk_piece: unknown token id, 값\n",
    "* bos_id, bos_piece: begin of sentence token id, 값\n",
    "* eos_id, eos_piece: end of sequence token id, 값\n",
    "* user_defined_symbols: 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233db45",
   "metadata": {},
   "source": [
    "* 토크나이저의 학습: 학습 문장들을 토대로 문장을 쪼개는 방식을 학습하는 것\n",
    "* 텍스트 파일 -> 학습 -> model, vocab 파일\n",
    "  * 문장들이 나열된 텍스트 파일을 넣고 학습시키면 model, vocab 파일이 만들어지는데\n",
    "    * model이 실제 토크나이징을 하고,\n",
    "    * vocab은 토크나이징 할 때 참조하는 단어집합\n",
    "    * 해당 예시의 경우, botchan.txt 파일을 읽은 후, m.model과 m.vocab 파일을 생성하게 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d13902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2941 obj=8.95617 num_tokens=22741 num_tokens/piece=7.7324\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2941 obj=8.88103 num_tokens=22745 num_tokens/piece=7.73376\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2205 obj=9.26224 num_tokens=25461 num_tokens/piece=11.5469\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2205 obj=9.17719 num_tokens=25457 num_tokens/piece=11.5451\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2200 obj=9.17892 num_tokens=25475 num_tokens/piece=11.5795\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2200 obj=9.17823 num_tokens=25475 num_tokens/piece=11.5795\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8cf10",
   "metadata": {},
   "source": [
    "### model load\n",
    "* 토크나이징 진행에 앞서 생성한 model을 불러온다\n",
    "* model 파일을 로드하여 단어 시퀀스를 정수 시퀀스로 바꾸는 인코딩 작업이나 반대로 변환하는 디코딩 작업을 할 수 있다\n",
    "* 로딩이 잘 된 경우 'True'를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3d4145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1200cb",
   "metadata": {},
   "source": [
    "### tokenize(tokenize 수행하기)\n",
    "#### encode\n",
    "* encode : 문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환 가능합니다.\n",
    "1. encode_as_pieces : string으로 tokenize, 문장을 입력하면 서브 워드 시퀀스로 변환합니다.\n",
    "2. encode_as_ids : ids으로 tokenize, 문장을 입력하면 정수 시퀀스로 변환합니다.\n",
    "\n",
    "#### decode\n",
    "* DecodeIds : 정수 시퀀스로부터 문장으로 변환합니다.\n",
    "* DecodePieces : 서브워드 시퀀스로부터 문장으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d078d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n",
      "[209, 31, 9, 375, 586]\n",
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
    "print(sp.decode_ids([209, 31, 9, 375, 586]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66ce83",
   "metadata": {},
   "source": [
    "결과 해석: {단어: id} 대응값을 사전으로 저장한 것이 아까 생성된 m.vocab 파일\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa40e5",
   "metadata": {},
   "source": [
    "### 그 외 기능들\n",
    "GetPieceSize() : 단어 집합의 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8869d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a960a16",
   "metadata": {},
   "source": [
    "1. idToPiece : 정수로부터 맵핑되는 서브 워드로 변환합니다.\n",
    "2. PieceToId : 서브워드로부터 맵핑되는 정수로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e83dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁This\n",
      "209\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# id <=> piece conversion\n",
    "print(sp.id_to_piece(209))\n",
    "print(sp.piece_to_id('▁This'))\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(sp.piece_to_id('__MUST_BE_UNKNOWN__')) # 알수 없는 토큰의 경우 0을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5220dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "    print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6753f28",
   "metadata": {},
   "source": [
    "## Changing the vocab id and surface representation of UNK/BOS/EOS/PAD symbols\n",
    "By default, UNK/BOS/EOS/PAD tokens and their ids are defined as follows:\n",
    "\n",
    "|token|UNK|BOS|EOS|PAD| ---|--- |surface|<unk>|<s>|</s>|<pad>| |id|0|1|2|undefined (-1)|\n",
    "\n",
    "We can change these mappings with --{unk|bos|eos|pad}_id and --{unk|bos|eos|pad}_piece flags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c63b5",
   "metadata": {},
   "source": [
    "센텐스피스로 단어 집합과 각 단어에 고유한 정수를 부여하기\n",
    "* input : 학습시킬 파일\n",
    "* vocab_size : 단어 집합의 크기\n",
    "* model_prefix : 만들어질 모델 이름\n",
    "* pad_id, pad_piece: pad token id, 값\n",
    "* unk_id, unk_piece: unknown token id, 값\n",
    "* bos_id, bos_piece: begin of sentence token id, 값\n",
    "* eos_id, eos_piece: end of sequence token id, 값\n",
    "\n",
    "------\n",
    "* model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
    "* max_sentence_length: 문장의 최대 길이\n",
    "* user_defined_symbols: 사용자 정의 토큰 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "483f3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] True\n",
      "[UNK] False\n",
      "[BOS] True\n",
      "[EOS] True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2941 obj=8.95617 num_tokens=22741 num_tokens/piece=7.7324\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2941 obj=8.88103 num_tokens=22745 num_tokens/piece=7.73376\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2205 obj=9.26224 num_tokens=25461 num_tokens/piece=11.5469\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2205 obj=9.17719 num_tokens=25457 num_tokens/piece=11.5451\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2200 obj=9.17892 num_tokens=25475 num_tokens/piece=11.5795\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2200 obj=9.17823 num_tokens=25475 num_tokens/piece=11.5795\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "\n",
    "for id in range(4):\n",
    "    print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f8eb1",
   "metadata": {},
   "source": [
    "## BPE (Byte pair encoding) model\n",
    "Sentencepiece는 --model_type=bpe flag로 지정해 subword segmentation을 위한 BPE (byte-pair-encoding)를 지원한다. \n",
    "* unigram과의 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "397718e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁this', 'is', 'at', 'est', 'he', 'llow', 'or', 'ld']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokeni"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')\n",
    "\n",
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces('thisisatesthelloworld'))\n",
    "print(sp_bpe.nbest_encode_as_pieces('hello world', 5))  # returns an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759d6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Unigram ***\n",
      "['▁this', 'is', 'ate', 's', 'the', 'llow', 'or', 'l', 'd']\n",
      "[['▁this', 'is', 'ate', 's', 'the', 'llow', 'or', 'l', 'd'], ['▁this', 'i', 's', 'ate', 's', 'the', 'llow', 'or', 'l', 'd'], ['▁this', 'is', 'ate', 'st', 'he', 'llow', 'or', 'l', 'd'], ['▁this', 'is', 'at', 'es', 'the', 'llow', 'or', 'l', 'd'], ['▁this', 'is', 'at', 'est', 'he', 'llow', 'or', 'l', 'd']]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.load('m_unigram.model')\n",
    "\n",
    "print('*** Unigram ***')\n",
    "print(sp_unigram.encode_as_pieces('thisisatesthelloworld'))\n",
    "print(sp_unigram.nbest_encode_as_pieces('thisisatesthelloworld', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aecf4cb",
   "metadata": {},
   "source": [
    "## Character and word model\n",
    "Sentencepiece를 통해\n",
    "character는 --model_type=char로,\n",
    "word는 --model_type=character로 segmentation이 가능하다\n",
    "\n",
    "In word segmentation에서 sentencepiece는 tokens를 그저 whitespaces로 segment하기 때문에,input text는 pre-tokenized되어야 한다. \n",
    "\n",
    "아래 결과 두 개 비교!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ad4a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 't', 'h', 'i', 's', '▁', 'i', 's', '▁', 'a', '▁', 't', 'e', 's', 't', '.']\n",
      "[3, 5, 10, 9, 11, 3, 9, 11, 3, 7, 3, 5, 4, 11, 5, 23]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_char --model_type=char --vocab_size=400')\n",
    "\n",
    "sp_char = spm.SentencePieceProcessor()\n",
    "sp_char.load('m_char.model')\n",
    "\n",
    "print(sp_char.encode_as_pieces('this is a test.'))\n",
    "print(sp_char.encode_as_ids('this is a test.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45882c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁test.']\n",
      "[31, 17, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('this is a test.'))  # '.' will not be one token.\n",
    "print(sp_word.encode_as_ids('this is a test.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8760b8",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "Sentencepiece 는 일반적인 사전 정의된 normalization rules를 제공한다. normalizer를 --normaliation_rule_name=<NAME> flag를 이용해 변경 가능하다.\n",
    "\n",
    "* nmt_nfkc: NFKC normalization with some additional normalization around spaces. (default)\n",
    "* nfkc: original: NFKC normalization.\n",
    "* nmt_nfkc_cf: nmt_nfkc + Unicode case folding (mostly lower casing)\n",
    "* nfkc_cf: nfkc + Unicode case folding.\n",
    "* identity: no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a301804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'hello', '▁world', '.']\n"
     ]
    }
   ],
   "source": [
    "# NFKC normalization and lower casing.\n",
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_name=nfkc_cf')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('ＨＥＬＬＯ　ＷＯＲＬＤ.'))  # lower casing and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995656ea",
   "metadata": {},
   "source": [
    "The normalization is performed with user-defined string-to-string mappings and leftmost longest matching. We can also define the custom normalization rules as TSV file. The TSV files for pre-defined normalziation rules can be found in the data directory (sample). The normalization rule is compiled into FST and embedded in the model file. We don't need to specify the normalization configuration in the segmentation phase.\n",
    "\n",
    "Here's the example of custom normalization. The TSV file is fed with --normalization_rule_tsv=<FILE> flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cd1f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+49 U+27 U+6d\tU+49 U+20 U+61 U+6d\n",
      "U+64 U+6f U+6e U+27 U+74\tU+64 U+6f U+20 U+6e U+6f U+74\n",
      "\n",
      "['▁I', '▁am', '▁bu', 's', 'y']\n",
      "['▁I', '▁do', '▁not', '▁know', '▁it', '.']\n"
     ]
    }
   ],
   "source": [
    "def tocode(s):\n",
    "    out = []\n",
    "    for c in s:\n",
    "        out.append(str(hex(ord(c))).replace('0x', 'U+'))\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "# TSV format:  source Unicode code points <tab> target code points\n",
    "# normalize \"don't => do not,  I'm => I am\"\n",
    "with open('normalization_rule.tsv', 'w') as f:\n",
    "  f.write(tocode(\"I'm\") + '\\t' + tocode(\"I am\") + '\\n')\n",
    "  f.write(tocode(\"don't\") + '\\t' + tocode(\"do not\") + '\\n')\n",
    "\n",
    "print(open('normalization_rule.tsv', 'r').read())\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_tsv=normalization_rule.tsv')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# m.model embeds the normalization rule compiled into an FST.\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces(\"I'm busy\"))  # normalzied to `I am busy'\n",
    "print(sp.encode_as_pieces(\"I don't know it.\"))  # normalized to 'I do not know it.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902534b",
   "metadata": {},
   "source": [
    "## Randomizing training data\n",
    "* Sentencepiece는 모든 training data를 model을 훈련하는데 load한다.\n",
    "* 하지만 training data가 크면, training time과 memory usage가 증가한다.\n",
    "* 만일 --input_sentence_size=<SIZE> 가 명시되어 있다면, Sentencepiece 랜덤하게 전체 훈련 데이터에서 <SIZE> lines를 샘플한다.\n",
    "* --shuffle_input_sentence=false는 랜덤 셔플을 불가하게 하고 first <SIZE> lines를 취하게 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f7dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁this', '▁is', '▁a', '▁t', 'est', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --input_sentence_size=1000')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "sp.encode_as_pieces('this is a test.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyolim_py38",
   "language": "python",
   "name": "hyolim_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
